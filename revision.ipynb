{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scopus_p1=pd.read_csv('data/Scopus_P1.csv')\n",
    "df_scopus_p2=pd.read_csv('data/Scopus_P2.csv')\n",
    "df_scopus_p3=pd.read_csv('data/Scopus_P3.csv')\n",
    "df_scopus_p4=pd.read_csv('data/Scopus_P4.csv')\n",
    "\n",
    "df_ieee_p1=pd.read_csv('data/IEEE_P1.csv')\n",
    "df_ieee_p2=pd.read_csv('data/IEEE_P2.csv')\n",
    "df_ieee_p3=pd.read_csv('data/IEEE_P3.csv')\n",
    "df_ieee_p4=pd.read_csv('data/IEEE_P4.csv')\n",
    "\n",
    "df_wos_p1=pd.read_csv('data/WoS_P1.csv')\n",
    "df_wos_p2=pd.read_csv('data/WoS_P2.csv')\n",
    "df_wos_p3=pd.read_csv('data/WoS_P3.csv')\n",
    "df_wos_p4=pd.read_csv('data/WoS_P4.csv')\n",
    "\n",
    "#\"Key\",\"Item Type\",\"Publication Year\",\"Author\",\"Title\",\"Publication Title\",\"ISBN\",\"ISSN\",\"DOI\",\"Url\",\"Abstract Note\"\n",
    "\n",
    "columns=['Key','Item Type','Publication Year','Author','Title','Publication Title','ISBN','ISSN','DOI','Url','Abstract Note']\n",
    "\n",
    "df_scopus_p1 = df_scopus_p1[columns]\n",
    "df_scopus_p2 = df_scopus_p2[columns]\n",
    "df_scopus_p3 = df_scopus_p3[columns]\n",
    "df_scopus_p4 = df_scopus_p4[columns]\n",
    "\n",
    "df_ieee_p1 = df_ieee_p1[columns]\n",
    "df_ieee_p2 = df_ieee_p2[columns]\n",
    "df_ieee_p3 = df_ieee_p3[columns]\n",
    "df_ieee_p4 = df_ieee_p4[columns]\n",
    "\n",
    "df_wos_p1 = df_wos_p1[columns]\n",
    "df_wos_p2 = df_wos_p2[columns]\n",
    "df_wos_p3 = df_wos_p3[columns]\n",
    "df_wos_p4 = df_wos_p4[columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined=pd.concat([df_scopus_p1,df_scopus_p2,df_scopus_p3,df_scopus_p4,df_ieee_p1,df_ieee_p2,df_ieee_p3,df_ieee_p4,df_wos_p1,df_wos_p2,df_wos_p3,df_wos_p4])\n",
    "\n",
    "#Order by title\n",
    "df_joined = df_joined.sort_values(by=['Title'])\n",
    "\n",
    "print(\"Before dropping duplicates by title: {}\".format(df_joined.shape))\n",
    "# Sort by 'Url' column, rows with NaN values will be first\n",
    "df_joined = df_joined.sort_values('Url', na_position='first')\n",
    "# Drop duplicates, keep the last occurrence (which will be the one with a URL if it exists)\n",
    "df_joined = df_joined.drop_duplicates(subset=['Title'], keep='last')\n",
    "print(\"After dropping duplicates by title: {}\".format(df_joined.shape))\n",
    "\n",
    "#add extra column with lower case title\n",
    "\n",
    "df_joined['Title_lower'] = df_joined['Title'].str.lower()\n",
    "\n",
    "#Remove duplicates by title_lower\n",
    "\n",
    "print(\"Before dropping duplicates by title_lower: {}\".format(df_joined.shape))\n",
    "df_joined = df_joined.drop_duplicates(subset=['Title_lower'])\n",
    "print(\"After dropping duplicates by title_lower: {}\".format(df_joined.shape))\n",
    "\n",
    "#Remove duplicates by DOI\n",
    "\n",
    "# print(\"Before dropping duplicates by DOI: {}\".format(df_joined.shape))\n",
    "# df_joined = df_joined.drop_duplicates(subset=['DOI'])\n",
    "# print(\"After dropping duplicates by DOI: {}\".format(df_joined.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop ISSN and ISBN\n",
    "df_joined = df_joined.drop(['ISSN','ISBN'], axis=1)\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to csv\n",
    "df_joined = df_joined.sort_values(by=['Title', 'Author'])\n",
    "df_joined.to_csv('data/merged.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuentes=pd.read_csv('data\\Formulario de extracción - Filtrado.csv')\n",
    "\n",
    "criterios=df_fuentes.columns[9:]\n",
    "df_fuentes[criterios]=df_fuentes[criterios].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of times that a row has a 1 in any of the criterias\n",
    "df_fuentes['Criterios']=df_fuentes[criterios].sum(axis=1)\n",
    "df_fuentes[df_fuentes['Criterios']==0].to_csv('data/filtered.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incluir ACL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Bib file from data/anthology+abstracts.bib\n",
    "\n",
    "#Read file\n",
    "import bibtexparser\n",
    "library = bibtexparser.parse_file('data/anthology+abstracts.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of entries\n",
    "print(\"Number of entries: {}\".format(len(library.entries)))\n",
    "\n",
    "#93386 entries in ACL Anthology\n",
    "#We get 92942\n",
    "\n",
    "#Get list of different fields\n",
    "fields=[]\n",
    "for entry in library.entries:\n",
    "    fields+=entry.fields_dict.keys()\n",
    "\n",
    "fields=set(fields)\n",
    "print(\"Fields: {}\".format(fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe with the fields as columns\n",
    "fields=list(fields)\n",
    "df_bib=pd.DataFrame(columns=fields)\n",
    "#Fill the dataframe with the entries\n",
    "# Fill the list with the entries\n",
    "data = []\n",
    "i=0\n",
    "for entry in library.entries:\n",
    "    # print(\"Entry: {}\".format(i))\n",
    "    aux_dict=entry.fields_dict\n",
    "    #Add entry.\n",
    "    data.append(entry.fields_dict)\n",
    "    #Add new pair to the just added dictionary. Key: 'entry_type', Value: entry.entry_type\n",
    "    data[i]['entry_type']=entry.entry_type\n",
    "    i+=1\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df_bib = pd.DataFrame(data)\n",
    "\n",
    "df_bib.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each entry in df_bib, only conserve its value if its of Field type\n",
    "df_bib = df_bib.applymap(lambda x: x.value if isinstance(x, bibtexparser.model.Field) else x)\n",
    "\n",
    "mapping_dict = {\n",
    "    'title': 'Title',\n",
    "    'author': 'Author',\n",
    "    'year': 'Publication Year',\n",
    "    'booktitle': 'Publication Title',\n",
    "    'doi': 'DOI',\n",
    "    'url': 'Url',\n",
    "    'abstract': 'Abstract Note',\n",
    "    'entry_type': 'Item Type'\n",
    "}\n",
    "\n",
    "df_bib = df_bib.rename(columns=mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before dropping before 2018: {}\".format(df_bib.shape))\n",
    "\n",
    "#Drop where year is null or before 2018\n",
    "df_bib = df_bib.dropna(subset=['Publication Year'])\n",
    "df_bib = df_bib[df_bib['Publication Year'].astype(int) >= 2018]\n",
    "\n",
    "print(\"After dropping before 2018: {}\".format(df_bib.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib = df_bib.dropna(subset=['Abstract Note'])\n",
    "df_bib.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to csv\n",
    "#Drop commas in abstracts and authors\n",
    "\n",
    "df_bib.to_csv('data/anthology+abstracts.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bib = pd.read_csv('data/anthology+abstracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to search this query in title or abstract\n",
    "#( \"NLP\" OR \"natural language processing\" ) AND (\"Low resource language?\" OR \"Endangered language?\" OR \"Indigenous language?\" OR \"Native language?\" OR \"Threatened language?\") AND (\"Text\" OR \"Sentences\" OR \"Words\") AND (\"Generat*\" OR \"Creat*\") AND (\"Architecture\" OR \"models\" OR \"network\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Define the search patterns\n",
    "# Define the search patterns\n",
    "patternsP1 = [\n",
    "    # r'\\b(nlp|natural language processing)\\b',\n",
    "    r'\\b(low resource languages?|endangered languages?|indigenous languages?|native languages?|threatened languages?)\\b',\n",
    "    r'\\b(texts?|sentences?|words?)\\b',\n",
    "    r'\\b(generat.*|creat.*)\\b',\n",
    "    r'\\b(architecture|models?|networks?)\\b'\n",
    "]\n",
    "\n",
    "patternsP2 = [\n",
    "    # r'\\b(nlp|natural language processing)\\b',\n",
    "    r'\\b(low resource languages?|endangered languages?|indigenous languages?|native languages?|threatened languages?)\\b',\n",
    "    r'\\b(texts?|sentences?|words?)\\b',\n",
    "    r'\\bdata\\b',\n",
    "    r'\\b(techniques?|methodolog.*|solutions?)\\b',\n",
    "    r'\\b(generat.*|creat.*)\\b'\n",
    "]\n",
    "\n",
    "patternsP3 = [\n",
    "    # r'\\b(nlp|natural language processing)\\b',\n",
    "    r'\\b(low resource languages?|endangered languages?|indigenous languages?|native languages?|threatened languages?)\\b',\n",
    "    r'\\b(texts?|sentences?|words?)\\b',\n",
    "    r'\\baugmenta.*\\b'\n",
    "]\n",
    "\n",
    "patternsP4 = [\n",
    "    # r'\\b(nlp|natural language processing)\\b',\n",
    "    r'\\b(low resource languages?|endangered languages?|indigenous languages?|native languages?|threatened languages?)\\b',\n",
    "    r'\\b(texts?|sentences?|words?)\\b',\n",
    "    r'\\b(generat.*|creat.*)\\b',\n",
    "    r'\\b(evaluat.*|assess.*|test.*)\\b'\n",
    "]\n",
    "\n",
    "# Apply the search patterns to the 'title' and 'abstract' columns\n",
    "df_bib['SearchP1'] = df_bib.apply(lambda row: all(re.search(pattern, row['Title'].lower() + ' ' + row['Abstract Note'].lower()) for pattern in patternsP1), axis=1)\n",
    "df_bib['SearchP2'] = df_bib.apply(lambda row: all(re.search(pattern, row['Title'].lower() + ' ' + row['Abstract Note'].lower()) for pattern in patternsP2), axis=1)\n",
    "df_bib['SearchP3'] = df_bib.apply(lambda row: all(re.search(pattern, row['Title'].lower() + ' ' + row['Abstract Note'].lower()) for pattern in patternsP3), axis=1)\n",
    "df_bib['SearchP4'] = df_bib.apply(lambda row: all(re.search(pattern, row['Title'].lower() + ' ' + row['Abstract Note'].lower()) for pattern in patternsP4), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib_p1 = df_bib[df_bib['SearchP1']] \n",
    "df_bib_p2 = df_bib[df_bib['SearchP2']]\n",
    "df_bib_p3 = df_bib[df_bib['SearchP3']]\n",
    "df_bib_p4 = df_bib[df_bib['SearchP4']]\n",
    "\n",
    "dfs = [df_bib_p1, df_bib_p2, df_bib_p3, df_bib_p4]\n",
    "for i, df in enumerate(dfs):\n",
    "    print(\"P{}: {}\".format(i+1, df.shape))\n",
    "    df.to_csv('data/anthology+abstracts_P{}.csv'.format(i+1),index=False)\n",
    "\n",
    "df_bib_concat=pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bib_concat.drop_duplicates(subset=['Title'], inplace=True)\n",
    "df_bib_concat.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv('data/filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toConserve=df_filtered.columns[1:9].tolist()\n",
    "df_bib_concat = df_bib_concat[toConserve]\n",
    "df_bib_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to csv\n",
    "df_bib_concat.to_csv('data/aclFiltrado.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.read_csv('data/Formulario de extracción - Formulario.csv')\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns\n",
    "renaming_dict = {\n",
    "    'Título': 'Title',\n",
    "    'Autor(es)': 'Author',\n",
    "    'Año': 'Publication Year',\n",
    "    'Revista/Conferencia': 'Publication Title',\n",
    "    'DOI': 'DOI',\n",
    "    'URL': 'Url',\n",
    "    'Resumen': 'Abstract Note',\n",
    "    'Tipo de publicación': 'Item Type',\n",
    "    'ID': 'Key',\n",
    "}\n",
    "\n",
    "renaming_dict = {v: k for k, v in renaming_dict.items()}\n",
    "df_final = df_final.rename(columns=renaming_dict)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace ID with REF# \n",
    "df_final['ID'] = range(1, len(df_final) + 1)\n",
    "df_final['ID'] = 'REF' + df_final['ID'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Tipo de publicación'].value_counts()\n",
    "renaming_dict = {\n",
    "    'Artículo en revista': 'journalArticle',\n",
    "    'Artículo en conferencia': 'conferencePaper',\n",
    "    'Libro': 'book',\n",
    "    'Capítulo de libro': 'bookSection',\n",
    "    'Tesis': 'thesis',\n",
    "    'Otro': 'other',\n",
    "}\n",
    "renaming_dict = {v: k for k, v in renaming_dict.items()}\n",
    "df_final['Tipo de publicación'] = df_final['Tipo de publicación'].replace(renaming_dict)\n",
    "df_final['Tipo de publicación'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('data/Formulario de extracción - Formulario.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
