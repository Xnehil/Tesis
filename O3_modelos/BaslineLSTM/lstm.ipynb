{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_xla.core.xla_model as xm\n",
    "\n",
    "# # List all available TPU devices\n",
    "# devices = xm.get_xla_supported_devices()\n",
    "# print(f'Available TPU devices: {devices}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class LSTMTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.3, pretrained_embeddings=None, device=None):\n",
    "        super(LSTMTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Initialize the embedding layer with pre-trained embeddings if provided\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = False  \n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)  \n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        num_layers = self.lstm.num_layers\n",
    "        hidden_size = self.lstm.hidden_size\n",
    "        num_directions = 2  \n",
    "        if self.device is not None:\n",
    "            return (torch.zeros(num_layers * num_directions, batch_size, hidden_size).to(self.device),\n",
    "                    torch.zeros(num_layers * num_directions, batch_size, hidden_size).to(self.device))\n",
    "        return (torch.zeros(num_layers * num_directions, batch_size, hidden_size),\n",
    "                torch.zeros(num_layers * num_directions, batch_size, hidden_size))\n",
    "\n",
    "# Example usage\n",
    "# device = xm.xla_device()\n",
    "# model = LSTMTextGenerator(vocab_size=5000, embed_size=300, hidden_size=256, num_layers=2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Load the vocabulary (token -> index) mapping\n",
    "vocab = {}  # You should populate this with your BPE vocabulary\n",
    "with open('tokenizadorIskonawa.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for idx, line in enumerate(vocab_file):\n",
    "        token, code = re.split(r'\\t', line.strip())\n",
    "        # Save as integer\n",
    "        vocab[token] = idx\n",
    "\n",
    "# Load the BPE tokenized dataset\n",
    "def load_bpe_dataset(file_path, vocab):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Tokenize each line into subwords and convert them to indices\n",
    "            tokens = line.strip().split()  # Assuming tokens are space-separated\n",
    "            token_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]  # Handle unknown tokens\n",
    "            dataset.append(token_ids)\n",
    "    return dataset\n",
    "\n",
    "bpe_tokenized_dataset = load_bpe_dataset('tokens.txt', vocab)\n",
    "\n",
    "# Check dataset example\n",
    "print(bpe_tokenized_dataset[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(embedding_file, vocab):\n",
    "    with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "        # Read the first line to get vocab size and embed size\n",
    "        first_line = f.readline().strip()\n",
    "        vocab_size, embed_size = map(int, first_line.split())\n",
    "        \n",
    "        # Initialize a dictionary to hold the embeddings\n",
    "        embeddings = np.zeros((len(vocab), embed_size), dtype=np.float32)\n",
    "        \n",
    "        # Read the rest of the file\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            subword = values[0].strip()\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            index = vocab.get(subword, -1)\n",
    "            if index == -1:\n",
    "                print(f'Found {subword} in vocab')\n",
    "            else:\n",
    "                embeddings[index] = vector\n",
    "\n",
    "\n",
    "    return embeddings, vocab_size, embed_size\n",
    "\n",
    "embedding_file = 'isk_anchor_final3.txt'\n",
    "pretrained_embeddings, vocab_size, embed_size = load_embeddings(embedding_file, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, pad_token=0):\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the tokenized sentence\n",
    "        sentence = self.tokenized_data[idx]\n",
    "        \n",
    "        # Convert to tensor and return\n",
    "        return torch.tensor(sentence, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_length = max(len(sentence) for sentence in batch)\n",
    "    padded_batch = [torch.cat([sentence, torch.tensor([0] * (max_length - len(sentence)))]) for sentence in batch]\n",
    "    return torch.stack(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BPEDataset(bpe_tokenized_dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Check example\n",
    "for inputs in dataloader:\n",
    "    print(inputs.shape)  # Check the shape of the padded input batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, checkpoint_dir='checkpoints'):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_uni_last.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, checkpoint_path)\n",
    "    print(f'Checkpoint saved at {checkpoint_path}')\n",
    "    # Create  or append to a file called \"checkpoint_log.txt\" to save the epoch and loss\n",
    "    with open('checkpoint_log.txt', 'a') as f:\n",
    "        f.write(f'Epoch: {epoch}, Loss: {loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f'Checkpoint loaded from {checkpoint_path}, epoch: {epoch}, loss: {loss}')\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f'No checkpoint found at {checkpoint_path}')\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.utils.utils as xu\n",
    "# Parameters\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "\n",
    "def train_loop_fn(loader, model, optimizer, criterion, epoch, device):\n",
    "    total_loss = 0\n",
    "    model.train()  # Set the model to training mode\n",
    "    step = 0\n",
    "    total_batches = len(loader)\n",
    "\n",
    "    for batch_idx, inputs in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare inputs and targets for text generation\n",
    "        inputs_seq = inputs[:, :-1].to(device).long()\n",
    "        targets_seq = inputs[:, 1:].to(device).long()\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = model.init_hidden(inputs_seq.size(0))  # Initialize hidden state based on batch size\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs_seq, hidden)  # Pass inputs_seq and hidden state\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets_seq.contiguous().view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer)\n",
    "        xm.mark_step()\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "        progress = (batch_idx + 1) / total_batches * 100  # Calculate progress percentage\n",
    "        print(f\"Successfully completed step {step} on device: {device}, Progress: {progress:.2f}%\")\n",
    "        step += 1\n",
    "\n",
    "    return total_loss / len(loader)  # Average loss over the epoch\n",
    "\n",
    "def _runa(rank, flags, device):\n",
    "    # Set up device\n",
    "    print(f'Training on: {device}\\n')\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = LSTMTextGenerator(vocab_size, embed_size, hidden_size, num_layers, pretrained_embeddings=pretrained_embeddings).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    start_epoch, _ = load_checkpoint('checkpoints/checkpoint_last.pth', model, optimizer)\n",
    "    if start_epoch is None:\n",
    "        start_epoch = 0\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        para_loader = pl.MpDeviceLoader(dataloader, device)\n",
    "        loss = train_loop_fn(para_loader, model, optimizer, criterion, epoch, device)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}, Device: {device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    device = xm.xla_device()\n",
    "    model = LSTMTextGenerator(vocab_size, embed_size, hidden_size, num_layers, pretrained_embeddings=pretrained_embeddings, device=device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Create the DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    start_epoch, _ = load_checkpoint('checkpoints/checkpoint_uni_last.pth', model, optimizer)\n",
    "    if start_epoch is None:\n",
    "        start_epoch = 0\n",
    "\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        para_loader = pl.ParallelLoader(dataloader, [device]).per_device_loader(device)\n",
    "        loss = train_loop_fn(para_loader, model, optimizer, criterion, epoch, device)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}, Device: {device}')\n",
    "        save_checkpoint(epoch, model, optimizer, loss)\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import gc\n",
    "import numpy as np\n",
    "# from sklearn import metrics\n",
    "\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001\n",
    "def _runa(rank, flags):\n",
    "    # Define training params \n",
    "    MAX_LEN = 192 # maximum text length in the batch (cannot have too high due to memory constraints)\n",
    "    BATCH_SIZE = 16 # batch size (cannot have too high due to memory constraints)\n",
    "    EPOCHS = 2 # number of epochs\n",
    "\n",
    "    # defining data samplers and loaders \n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "          dataset,\n",
    "          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n",
    "          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n",
    "          shuffle=True)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "        \n",
    "    # valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    #       valid_dataset,\n",
    "    #       num_replicas=xm.xrt_world_size(),\n",
    "    #       rank=xm.get_ordinal(),\n",
    "    #       shuffle=False)\n",
    "\n",
    "    # valid_data_loader = torch.utils.data.DataLoader(\n",
    "    #     valid_dataset,\n",
    "    #     batch_size=BATCH_SIZE,\n",
    "    #     sampler=valid_sampler,\n",
    "    #     drop_last=False,\n",
    "    #     num_workers=0\n",
    "    # )\n",
    "    \n",
    "\n",
    "    device = xm.xla_device() # our device (single TPU core)\n",
    "    model = LSTMTextGenerator(vocab_size, embed_size, hidden_size, num_layers, pretrained_embeddings=pretrained_embeddings).to(device)\n",
    "    xm.master_print(f'Training on: {device}\\n')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters()) # model parameters to optimize\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    # apply to weight decay\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    xm.master_print('training on train dataset')\n",
    "    \n",
    "    lr = 0.5e-5 * xm.xrt_world_size() # scale the learning rate\n",
    "    # calculate the total number of training steps\n",
    "    num_train_steps = int(len(dataset) / BATCH_SIZE / xm.xrt_world_size() * EPOCHS) \n",
    "    \n",
    "    \n",
    "    # a scheduler can be used if desired\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
    "\n",
    "    # Let's start training on the train set!\n",
    "    for epoch in range(EPOCHS):\n",
    "        gc.collect() # I use a lot of gc.collect() statement to hopefully prevent OOM problems\n",
    "        # We use ParallelLoader (provided by PyTorch XLA) for TPU-core-specific dataloading:\n",
    "        para_loader = pl.ParallelLoader(train_data_loader, [device]) \n",
    "        xm.master_print('parallel loader created... training now')\n",
    "        gc.collect()\n",
    "        # call training loop:\n",
    "        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, criterion, epoch, device)\n",
    "        del para_loader\n",
    "        # para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
    "        gc.collect()\n",
    "        # call evaluation loop\n",
    "        # o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n",
    "        # del para_loader\n",
    "        gc.collect()\n",
    "        # report AUC at the end\n",
    "        # auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n",
    "        # auc_reduced = xm.mesh_reduce('auc_reduce',auc,reduce_fn)\n",
    "        # xm.master_print(f'AUC = {auc_reduced}')\n",
    "        # gc.collect()\n",
    "    # save checkpoint\n",
    "    save_checkpoint(EPOCHS, model, optimizer, \"loss\", checkpoint_dir='checkpoints')\n",
    "    \n",
    "    # We can also repeat the process on the validation set as demonstrated by @xhlulu\n",
    "    \n",
    "    # xm.master_print('training on validation set')\n",
    "    \n",
    "    # lr = 1.5e-5 * xm.xrt_world_size()\n",
    "    \n",
    "    # num_train_steps = int(len(valid_dataset) / BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n",
    "    \n",
    "    # optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    # scheduler = get_linear_schedule_with_warmup(\n",
    "    #     optimizer,\n",
    "    #     num_warmup_steps=0.1*num_train_steps,\n",
    "    #     num_training_steps=num_train_steps\n",
    "    # )\n",
    "    # xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    flags = {}\n",
    "    xmp.spawn(_runa, args=(flags,), nprocs=8, start_method='fork')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_sequence, generation_length, seq_len, device):\n",
    "    model.eval()\n",
    "    generated_sequence = start_sequence\n",
    "\n",
    "    # Initialize the hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = (hidden[0].to(device), hidden[1].to(device))  # Move each element to device\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(generation_length):\n",
    "            input_seq = torch.tensor(generated_sequence[-(seq_len-1):], dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            # print(output.shape) \n",
    "            next_token = output.argmax(dim=2)[:,-1].item()\n",
    "            generated_sequence.append(next_token)\n",
    "    \n",
    "    return generated_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from checkpoints/checkpoint_last.pth, epoch: 19, loss: 8.059777471754286\n",
      "Loaded model from epoch 19 with loss 8.059777471754286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_81223/1052079623.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "device = xm.xla_device()\n",
    "model = LSTMTextGenerator(vocab_size, embed_size, hidden_size, num_layers, pretrained_embeddings=pretrained_embeddings).to(device)\n",
    "start_sequence = [vocab['▁ma']]  # Start with the token for 'ma'\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epoch, loss=load_checkpoint('checkpoints/checkpoint_last.pth', model, optimizer)\n",
    "print(\"Loaded model from epoch {} with loss {}\".format(epoch, loss))\n",
    "\n",
    "import sentencepiece as spm\n",
    "tokenizerIsk = spm.SentencePieceProcessor(model_file='tokenizadorIskonawa.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=generate_text(model, start_sequence=[vocab['ke']], generation_length=20, seq_len=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ke',\n",
       " 'pó',\n",
       " '▁pae',\n",
       " '▁pae',\n",
       " '▁pae',\n",
       " '▁pae',\n",
       " '▁pae',\n",
       " '▁pae',\n",
       " '▁pae',\n",
       " '▁pae',\n",
       " '▁pae',\n",
       " 'én',\n",
       " 'én',\n",
       " '▁hanawe',\n",
       " 'én',\n",
       " '▁hanawe',\n",
       " 'én',\n",
       " 'én',\n",
       " '▁hanawe',\n",
       " 'én',\n",
       " '▁hanawe']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequenceA = [tokenizerIsk.id_to_piece(idx) for idx in sequence]\n",
    "sequenceA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
