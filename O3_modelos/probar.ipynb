{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación\n",
    "\n",
    "Oraciones originales de referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "corpus=pd.read_json(\"../O1_Corpus/corpus.json\", lines=True)\n",
    "\n",
    "def etapa_preprocesamiento(textos, tokenizador=None):\n",
    "    \n",
    "    #Textos es una columna de un dataframe\n",
    "    #1. Pasar a minúsculas\n",
    "    textos = textos.str.lower()\n",
    "    #2. Eliminar caracteres especiales\n",
    "    textos = textos.apply(lambda x: re.sub(r\"[\\W\\d_]+\", \" \", x))\n",
    "    textos = textos.apply(lambda x: re.sub(r\"ininteligible\", \"\", x))\n",
    "    #3. Eliminar espacios en blanco extra\n",
    "    textos = textos.apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "    #4. Eliminar espacios en blanco al principio y al final\n",
    "    textos = textos.str.strip()\n",
    "    #5. Tokenizar usando SentencePiece\n",
    "    if tokenizador:\n",
    "        textos = textos.apply(lambda x: tokenizador.encode_as_pieces(x))\n",
    "    return textos\n",
    "\n",
    "corpus['transcription'] = etapa_preprocesamiento(corpus['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_texts=corpus.sample(300)['transcription'].to_list()\n",
    "reference_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mauve\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, input_texts, device):\n",
    "    model.eval()  \n",
    "    total_loss = 0\n",
    "    for text in input_texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(input_texts)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def calculate_perplexity_lstm(model, vocab, input_texts, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.get('<unk>', -100))\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in input_texts:\n",
    "            # Tokenize the input text and shift targets\n",
    "            token_ids = [vocab.get(token, vocab['<unk>']) for token in text.split()]\n",
    "            if len(token_ids) < 2:  # Ensure there's enough context for prediction\n",
    "                continue\n",
    "            \n",
    "            inputs = torch.tensor(token_ids[:-1], dtype=torch.long).unsqueeze(0).to(device)  # Exclude last token\n",
    "            targets = torch.tensor(token_ids[1:], dtype=torch.long).unsqueeze(0).to(device)  # Exclude first token\n",
    "\n",
    "            # Reinitialize the hidden state\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            # Calculate the loss, skip if NaN\n",
    "            loss = criterion(outputs, targets)\n",
    "            if not torch.isnan(loss):\n",
    "                total_loss += loss.item() * targets.size(0)\n",
    "                total_tokens += targets.size(0)\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "def calculate_distinct_n(generated_texts, n=1):\n",
    "    n_grams = []\n",
    "    for text in generated_texts:\n",
    "        # Split the text into characters\n",
    "        chars = list(text)\n",
    "        # Generate n-grams from characters\n",
    "        n_grams.extend([tuple(chars[i:i+n]) for i in range(len(chars)-n+1)])\n",
    "\n",
    "    total_n_grams = len(n_grams)\n",
    "    unique_n_grams = len(set(n_grams))\n",
    "\n",
    "    distinct_n_score = unique_n_grams / total_n_grams if total_n_grams > 0 else 0\n",
    "    return distinct_n_score\n",
    "\n",
    "def calculate_mauve(generated_texts, reference_texts=reference_texts):\n",
    "    \"\"\"\n",
    "    Function to calculate the MAUVE score for generated texts.\n",
    "    \n",
    "    Args:\n",
    "    generated_texts (list): List of generated texts.\n",
    "    reference_texts (list): List of constant reference texts (human-written).\n",
    "    \n",
    "    Returns:\n",
    "    float: MAUVE score.\n",
    "    \"\"\"\n",
    "    cudaAvailable = torch.cuda.is_available()\n",
    "    print(f\"Using {'cuda' if cudaAvailable else 'cpu'}\")\n",
    "    mauve_score = mauve.compute_mauve(\n",
    "        p_text=generated_texts, \n",
    "        q_text=reference_texts, \n",
    "        device_id=0 if cudaAvailable else -1,\n",
    "        max_text_length=256\n",
    "    )\n",
    "    return mauve_score.mauve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each key in the dictionary, create a file with the key name and write each line in the value array\n",
    "for key in diccionarioGenerado:\n",
    "    with open(f\"{key}.txt\", \"w\") as file:\n",
    "        for line in diccionarioGenerado[key]:\n",
    "            file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "for transcription in corpus['transcription']:\n",
    "    words = transcription.split()\n",
    "    unique_words.update(words)\n",
    "\n",
    "unique_words_list = sorted(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZmBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "import sentencepiece as spm\n",
    "# Path to your model checkpoint\n",
    "model_path = \"checkpoints/zmbart\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizerIsk = spm.SentencePieceProcessor(model_file=model_path+\"/spiece.model\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\").to(device)\n",
    "checkpoint = torch.load(model_path + \"/zmbart_checkpoint112.pt\")\n",
    "model_weights = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
    "model.load_state_dict(model_weights, strict=False)\n",
    "\n",
    "def safe_decode(tokenizer, output_ids):\n",
    "    decoded_tokens = []\n",
    "    \n",
    "    for token_id in output_ids:\n",
    "        #Skip 0 token\n",
    "        if token_id.item() == 0:\n",
    "            continue\n",
    "        try:\n",
    "            # Decode each token individually. Now tokenizer is a SentencePiece processor\n",
    "            decoded_token = tokenizer.decode_ids([token_id.item()])\n",
    "            \n",
    "            # If token is padding or unknown, append an empty string or placeholder\n",
    "            if decoded_token == \"<pad>\" or decoded_token == \"<unk>\" or decoded_token == \"⁇\":\n",
    "                decoded_tokens.append(\"\")\n",
    "            else:\n",
    "                decoded_tokens.append(decoded_token)\n",
    "        except IndexError:\n",
    "            # Handle index errors by appending an empty string\n",
    "            decoded_tokens.append(\"\")\n",
    "    \n",
    "    # Join tokens with spaces, replacing empty tokens appropriately\n",
    "    return ' '.join(decoded_tokens).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_samples = 1\n",
    "sentences = []\n",
    "\n",
    "while len(sentences) <= 100:\n",
    "    input_texts = random.sample(unique_words_list, num_samples)\n",
    "    # print(\"Input texts: \", input_texts)\n",
    "    \n",
    "    # Encode the input texts\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    # print(\"Encoded inputs: \", inputs)\n",
    "    \n",
    "    # Generate outputs\n",
    "    outputs = model.generate(**inputs, max_length=20, num_beams=10, early_stopping=False, no_repeat_ngram_size=1, pad_token_id=tokenizer.pad_token_id)\n",
    "    # print(\"Outputs: \", outputs)\n",
    "    \n",
    "    # Decode the outputs\n",
    "    decoded_outputs = [safe_decode(tokenizerIsk, output) for output in outputs]\n",
    "    # print(\"Decoded outputs: \", decoded_outputs)\n",
    "    sentences.extend(decoded_outputs)\n",
    "    print(\"Tenemos \", len(sentences), \" oraciones\")\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado[\"zmbart\"]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = calculate_perplexity(model, tokenizer, sentences, device)\n",
    "distinct_2 = calculate_distinct_n(sentences, n=2)\n",
    "distinct_3 = calculate_distinct_n(sentences, n=3)\n",
    "mauve_score = calculate_mauve(sentences)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Distinct-2: {distinct_2}\")\n",
    "print(f\"Distinct-3: {distinct_3}\")\n",
    "print(f\"MAUVE: {mauve_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: 43.40846252441406\n",
    "Distinct-2: 0.03824678950307091\n",
    "Distinct-3: 0.11806952025280092\n",
    "MAUVE: 0.031570415352922064"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta XNLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Path to your model checkpoint\n",
    "checkpoint_path = \"/workspace/Tesis/O3_modelos/checkpoints/metaXNLG_checkpoint-10500/\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
    "\n",
    "def safe_decode(tokenizer, output_ids):\n",
    "    # print(output_ids)\n",
    "    decoded_tokens = []\n",
    "    \n",
    "    for token_id in output_ids:\n",
    "        try:\n",
    "            # Decode each token individually\n",
    "            decoded_token = tokenizer.decode(token_id.item(), skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "            \n",
    "            # If token is padding or unknown, append an empty string or placeholder\n",
    "            if decoded_token == \"<pad>\" or decoded_token == \"<unk>\":\n",
    "                decoded_tokens.append(\"\")\n",
    "            else:\n",
    "                decoded_tokens.append(decoded_token)\n",
    "        except IndexError:\n",
    "            # Handle index errors by appending an empty string\n",
    "            decoded_tokens.append(\"\")\n",
    "    \n",
    "    # Join tokens with spaces, replacing empty tokens appropriately\n",
    "    return ' '.join(decoded_tokens).strip().replace('▁', ' ').replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a number of input texts\n",
    "num_samples = 1\n",
    "import random\n",
    "\n",
    "\n",
    "sentences = []\n",
    "\n",
    "while len(sentences) <= 100:\n",
    "    input_texts = random.sample(unique_words_list, num_samples) \n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Move inputs to GPU\n",
    "    outputs = model.generate(**inputs, max_length=30)\n",
    "    predictions = [safe_decode(tokenizer, output) for output in outputs]\n",
    "    split_sentences = [sentence.split('</s>') for sentence in predictions]\n",
    "    split_sentences = [[s.strip() for s in sentence_list if s.strip()] for sentence_list in split_sentences]\n",
    "    flattened_sentences = [item for sublist in split_sentences for item in sublist]\n",
    "    sentences.extend(flattened_sentences)\n",
    "    print(\"Tenemos \",len(sentences),\" oraciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado[\"metaXNLG\"]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas\n",
    "perplexity = calculate_perplexity(model, tokenizer, sentences, device)\n",
    "distinct_2 = calculate_distinct_n(sentences, n=2)\n",
    "distinct_3 = calculate_distinct_n(sentences, n=3)\n",
    "mauve_score = calculate_mauve(sentences)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Distinct-2: {distinct_2}\")\n",
    "print(f\"Distinct-3: {distinct_3}\")\n",
    "print(f\"MAUVE: {mauve_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: 53.81763458251953\n",
    "Distinct-2: 0.06568575932737783\n",
    "Distinct-3: 0.20421753607103219\n",
    "MAUVE: 0.5293356144783942"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define the LSTMTextGenerator class\n",
    "\n",
    "class LSTMTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.3, pretrained_embeddings=None, device=None):\n",
    "        super(LSTMTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Initialize the embedding layer with pre-trained embeddings if provided\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = False  \n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)  \n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        num_layers = self.lstm.num_layers\n",
    "        hidden_size = self.lstm.hidden_size\n",
    "        num_directions = 2  \n",
    "        if self.device is not None:\n",
    "            return (torch.zeros(num_layers * num_directions, batch_size, hidden_size).to(self.device),\n",
    "                    torch.zeros(num_layers * num_directions, batch_size, hidden_size).to(self.device))\n",
    "        return (torch.zeros(num_layers * num_directions, batch_size, hidden_size),\n",
    "                torch.zeros(num_layers * num_directions, batch_size, hidden_size))\n",
    "\n",
    "# Load the vocabulary (token -> index) mapping\n",
    "vocab = {}\n",
    "with open('BaslineLSTM/tokenizadorIskonawa.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for idx, line in enumerate(vocab_file):\n",
    "        token, code = re.split(r'\\t', line.strip())\n",
    "        vocab[token] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "model = LSTMTextGenerator(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = \"checkpoints/lstm/checkpoint_last.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_embeddings(embedding_file, vocab):\n",
    "#     with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "#         # Read the first line to get vocab size and embed size\n",
    "#         first_line = f.readline().strip()\n",
    "#         vocab_size, embed_size = map(int, first_line.split())\n",
    "        \n",
    "#         # Initialize a dictionary to hold the embeddings\n",
    "#         embeddings = np.zeros((len(vocab), embed_size), dtype=np.float32)\n",
    "        \n",
    "#         # Read the rest of the file\n",
    "#         for line in f:\n",
    "#             values = line.strip().split()\n",
    "#             subword = values[0].strip()\n",
    "#             vector = np.array(values[1:], dtype=np.float32)\n",
    "#             index = vocab.get(subword, -1)\n",
    "#             if index == -1:\n",
    "#                 print(f'Found {subword} in vocab')\n",
    "#             else:\n",
    "#                 embeddings[index] = vector\n",
    "    \n",
    "#     return embeddings, vocab_size, embed_size\n",
    "\n",
    "# embedding_file = 'BaslineLSTM/isk_anchor_final2.txt'\n",
    "# pretrained_embeddings, vocab_size, embed_size = load_embeddings(embedding_file, vocab)\n",
    "\n",
    "# model.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def test_lstm(model, input_data, vocab, max_length=8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        inputs = torch.tensor([[vocab[token] for token in sentence] for sentence in input_data]).to(device)\n",
    "        hidden = model.init_hidden(inputs.size(0))\n",
    "        hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "        \n",
    "\n",
    "        generated_tokens = inputs.tolist()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            max_repeat = 2\n",
    "            # Generate predictions based on the input sequence and hidden state\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "            \n",
    "            # Append predicted token to generated tokens\n",
    "            generated_tokens[0].append(predictions[0, -1].item())\n",
    "            \n",
    "            if len(generated_tokens[0]) > 1 and generated_tokens[0][-1] == generated_tokens[0][-2]:\n",
    "                max_repeat -= 1\n",
    "                if max_repeat == 0:\n",
    "                    print(\"Se repitio\")\n",
    "                    break   \n",
    "                generated_tokens[0][-1] = random.choice(range(vocab_size))\n",
    "                \n",
    "        \n",
    "            # Update inputs with the latest predicted token\n",
    "            inputs = torch.cat((inputs, predictions[:, -1].unsqueeze(1)), dim=1)\n",
    "        \n",
    "        # Convert indices back to words, excluding '<unk>' tokens\n",
    "        generated_sentences = []\n",
    "        for tokens in generated_tokens:\n",
    "            sentence = [list(vocab.keys())[list(vocab.values()).index(token)] for token in tokens if token in vocab.values() and token != vocab['<unk>']]\n",
    "            generated_sentences.append(sentence)\n",
    "        \n",
    "    return generated_sentences\n",
    "\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    sentence=''.join(tokens)\n",
    "    return sentence.replace('▁', ' ').replace('  ', ' ').strip()\n",
    "\n",
    "# Generate sentences using the LSTM model\n",
    "num_samples = 2\n",
    "sentences = []\n",
    "reverse_vocab = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "while len(sentences) <= 100:\n",
    "    input_texts = random.sample(list(vocab.keys()), num_samples)\n",
    "    input_data = [input_texts]  # Wrap in a list to match the expected input format\n",
    "\n",
    "    # print(\"Input texts: \", input_texts)\n",
    "    predictions = test_lstm(model, input_data, vocab)\n",
    "    # print(\"Predictions: \", predictions)\n",
    " \n",
    "    flattened_sentences = [token for sentence in predictions for token in sentence]\n",
    "    # print(\"Flattened sentences: \", flattened_sentences)\n",
    "    joint_sentence=join_tokens(flattened_sentences)\n",
    "\n",
    "    sentences.extend([joint_sentence])\n",
    "    \n",
    "    # print(\"Tenemos \", len(sentences), \" oraciones\")\n",
    "    # break\n",
    "    # Break after processing one batch (for testing, remove this in real runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado[\"LSTM\"]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = calculate_perplexity_lstm(model, vocab, sentences, device)\n",
    "# distinct_2 = calculate_distinct_n(sentences, n=2)\n",
    "# distinct_3 = calculate_distinct_n(sentences, n=3)\n",
    "# mauve_score = calculate_mauve(sentences)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Distinct-2: {distinct_2}\")\n",
    "print(f\"Distinct-3: {distinct_3}\")\n",
    "print(f\"MAUVE: {mauve_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: 2473.0986328125\n",
    "Distinct-2: 0.0713372538992585\n",
    "Distinct-3: 0.2671916010498688\n",
    "MAUVE: 0.41212726326292703"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# Define paths to checkpoint and SentencePiece model\n",
    "MODEL_DIR = \"checkpoints/t5\"\n",
    "TF_CHECKPOINT_PATH = \"checkpoints/t5/smallT5_model.ckpt-120000\"\n",
    "SP_MODEL_PATH = \"checkpoints/t5/spiece.model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(SP_MODEL_PATH)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dense_proj = tf.keras.Sequential([\n",
    "            Dense(d_ff, activation='gelu'),\n",
    "            Dense(d_model),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        dense_output = self.dense_proj(out1)\n",
    "        dense_output = self.dropout2(dense_output, training=training)\n",
    "        return self.layernorm2(out1 + dense_output)\n",
    "\n",
    "class SimpleT5Model(tf.keras.Model):\n",
    "    def __init__(self, d_model, num_layers, num_heads, d_ff, dropout_rate):\n",
    "        super(SimpleT5Model, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Dense(d_model)\n",
    "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.final_layer = Dense(d_model)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        return self.final_layer(x)\n",
    "\n",
    "d_model = 512\n",
    "num_layers = 8\n",
    "d_ff = 1024\n",
    "num_heads = 6\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model = SimpleT5Model(d_model, num_layers, num_heads, d_ff, dropout_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Restore the model weights from the checkpoint\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "status = checkpoint.restore(TF_CHECKPOINT_PATH).expect_partial()\n",
    "\n",
    "# Confirm if checkpoint was successfully restored\n",
    "status.assert_existing_objects_matched()\n",
    "\n",
    "# Encode input text using SentencePiece\n",
    "def encode_text(text):\n",
    "    return sp.EncodeAsIds(text)\n",
    "\n",
    "# Decode output IDs to text using SentencePiece\n",
    "def decode_text(ids):\n",
    "    return sp.DecodeIds(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "num_samples = 1\n",
    "sentences = []\n",
    "\n",
    "while len(sentences) <= 100:\n",
    "    input_texts = random.sample(unique_words_list, num_samples)\n",
    "    inputs = [encode_text(text) for text in input_texts]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, padding='post', maxlen=15)\n",
    "    \n",
    "    # Ensure the input tensor has the correct shape\n",
    "    inputs = tf.convert_to_tensor(inputs, dtype=tf.int32)\n",
    "    inputs = tf.expand_dims(inputs, axis=-1)  # Add a new dimension if needed\n",
    "    inputs = tf.tile(inputs, [1, 1, 3])  # Tile to match the expected feature size\n",
    "    \n",
    "    outputs = model(inputs, training=False)\n",
    "    # print(outputs)\n",
    "    token_ids = tf.argmax(outputs, axis=-1).numpy().squeeze()\n",
    "    \n",
    "    # Ensure token_ids is a list of lists\n",
    "    if token_ids.ndim == 1:\n",
    "        token_ids = [token_ids]\n",
    "    \n",
    "    #Truncate token_ids to the second time the '454' token appears\n",
    "    token_ids = [ids[:np.where(ids == 454)[0][1]] if np.sum(ids == 454) > 1 else ids for ids in token_ids]\n",
    "\n",
    "    # print(token_ids)\n",
    "    # Decode the token IDs to text\n",
    "    predictions = [decode_text(ids.tolist()) for ids in token_ids]\n",
    "    \n",
    "    #Sentence is the input text and the prediction\n",
    "    sentence = input_texts[0] + ' ' + predictions[0]\n",
    "    \n",
    "    sentences.extend([sentence])\n",
    "    # print(\"Tenemos \",len(sentences),\" oraciones\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence.replace('▁', ' ').replace('  ', ' ').strip() for sentence in sentences]\n",
    "diccionarioGenerado[\"T5\"]=sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Define the cross-entropy loss\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Function to compute perplexity\n",
    "def calculate_perplexity_t5(model, input_texts):\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for text in input_texts:\n",
    "        # Tokenize and encode the input text\n",
    "        token_ids = encode_text(text)\n",
    "        #For those token_ids values bigger than 512, assign them a random value\n",
    "        token_ids = [token if token < 512 else random.choice(range(512)) for token in token_ids]\n",
    "        token_ids = tf.convert_to_tensor(token_ids, dtype=tf.int32)\n",
    "        \n",
    "        \n",
    "        if len(token_ids) < 2:  # Need at least 2 tokens to compute loss\n",
    "            continue\n",
    "\n",
    "        # Convert to TensorFlow constant and add batch dimension\n",
    "        inputs = tf.constant(token_ids, dtype=tf.int32)\n",
    "        inputs = tf.expand_dims(inputs, axis=0)\n",
    "\n",
    "        # Adjust `inputs` shape to match model requirements\n",
    "        if inputs.shape[-1] % 3 != 0:  # Check if padding is needed\n",
    "            padding_needed = 3 - (inputs.shape[-1] % 3)\n",
    "            inputs = tf.pad(inputs, [[0, 0], [0, padding_needed]])\n",
    "\n",
    "        # Reshape for the model if required\n",
    "        inputs = tf.reshape(inputs, (-1, inputs.shape[1] // 3, 3))\n",
    "\n",
    "        # Get model predictions\n",
    "        predictions = model(inputs, training=False)\n",
    "\n",
    "        # Prepare targets for calculating loss\n",
    "        targets = token_ids[1:]  # Shifted by one\n",
    "        targets = tf.constant(targets, dtype=tf.int32)\n",
    "\n",
    "        # If shapes don't align, pad `targets`\n",
    "        if targets.shape[0] < predictions.shape[1]:\n",
    "            targets = tf.pad(targets, [[0, predictions.shape[1] - targets.shape[0]]])\n",
    "        elif targets.shape[0] > predictions.shape[1]:\n",
    "            targets = targets[:predictions.shape[1]]  # Truncate to match\n",
    "\n",
    "        targets = tf.reshape(targets, predictions.shape[:-1])  # Match dimensions\n",
    "\n",
    "        # Calculate cross-entropy loss\n",
    "        loss = loss_fn(targets, predictions)\n",
    "        total_loss += loss.numpy() * len(targets)\n",
    "        total_tokens += len(targets)\n",
    "\n",
    "    # Calculate average loss per token\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    # Compute perplexity\n",
    "    perplexity = np.exp(avg_loss) if avg_loss > 0 else float('inf')\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/testeo/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 6, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|██████████| 101/101 [00:04<00:00, 20.73it/s]\n",
      "Featurizing q: 100%|██████████| 299/299 [00:12<00:00, 23.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 169.55085834426123\n",
      "Distinct-2: 0.07234479220112879\n",
      "Distinct-3: 0.19155844155844157\n",
      "MAUVE: 0.009045433531443093\n"
     ]
    }
   ],
   "source": [
    "#Métricas\n",
    "perplexity = calculate_perplexity_t5(model, sentences)\n",
    "distinct_2 = calculate_distinct_n(sentences, n=2)\n",
    "distinct_3 = calculate_distinct_n(sentences, n=3)\n",
    "mauve_score = calculate_mauve(sentences)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Distinct-2: {distinct_2}\")\n",
    "print(f\"Distinct-3: {distinct_3}\")\n",
    "print(f\"MAUVE: {mauve_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: 169.55085834426123\n",
    "Distinct-2: 0.07234479220112879\n",
    "Distinct-3: 0.19155844155844157\n",
    "MAUVE: 0.009045433531443093"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
