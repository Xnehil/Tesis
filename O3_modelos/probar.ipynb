{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación\n",
    "\n",
    "Oraciones originales de referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "corpus=pd.read_json(\"../O1_Corpus/corpus.json\", lines=True)\n",
    "\n",
    "def etapa_preprocesamiento(textos, tokenizador=None):\n",
    "    \n",
    "    #Textos es una columna de un dataframe\n",
    "    #1. Pasar a minúsculas\n",
    "    textos = textos.str.lower()\n",
    "    #2. Eliminar caracteres especiales\n",
    "    textos = textos.apply(lambda x: re.sub(r\"[\\W\\d_]+\", \" \", x))\n",
    "    textos = textos.apply(lambda x: re.sub(r\"ininteligible\", \"\", x))\n",
    "    #3. Eliminar espacios en blanco extra\n",
    "    textos = textos.apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "    #4. Eliminar espacios en blanco al principio y al final\n",
    "    textos = textos.str.strip()\n",
    "    #5. Tokenizar usando SentencePiece\n",
    "    if tokenizador:\n",
    "        textos = textos.apply(lambda x: tokenizador.encode_as_pieces(x))\n",
    "    return textos\n",
    "\n",
    "corpus['transcription'] = etapa_preprocesamiento(corpus['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_texts=corpus.sample(300)['transcription'].to_list()\n",
    "reference_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mauve\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, input_texts, device):\n",
    "    model.eval()  \n",
    "    total_loss = 0\n",
    "    for text in input_texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(input_texts)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "def calculate_perplexity_lstm(model, vocab, input_texts, device):\n",
    "    model.eval()  \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<unk>'])  \n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in input_texts:\n",
    "            # Tokenize the input text\n",
    "            token_ids = [vocab.get(token, vocab['<unk>']) for token in text.split()]\n",
    "            inputs = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(device)  \n",
    "            targets = inputs.clone()\n",
    "\n",
    "            # Initialize the hidden state\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            outputs = outputs.view(-1, outputs.size(-1)) \n",
    "            targets = targets.view(-1)  \n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            total_tokens += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "def calculate_distinct_n(generated_texts, n=1):\n",
    "    n_grams = []\n",
    "    for text in generated_texts:\n",
    "        # Split the text into characters\n",
    "        chars = list(text)\n",
    "        # Generate n-grams from characters\n",
    "        n_grams.extend([tuple(chars[i:i+n]) for i in range(len(chars)-n+1)])\n",
    "\n",
    "    total_n_grams = len(n_grams)\n",
    "    unique_n_grams = len(set(n_grams))\n",
    "\n",
    "    distinct_n_score = unique_n_grams / total_n_grams if total_n_grams > 0 else 0\n",
    "    return distinct_n_score\n",
    "\n",
    "def calculate_mauve(generated_texts, reference_texts=reference_texts):\n",
    "    \"\"\"\n",
    "    Function to calculate the MAUVE score for generated texts.\n",
    "    \n",
    "    Args:\n",
    "    generated_texts (list): List of generated texts.\n",
    "    reference_texts (list): List of constant reference texts (human-written).\n",
    "    \n",
    "    Returns:\n",
    "    float: MAUVE score.\n",
    "    \"\"\"\n",
    "    cudaAvailable = torch.cuda.is_available()\n",
    "    print(f\"Using {'cuda' if cudaAvailable else 'cpu'}\")\n",
    "    mauve_score = mauve.compute_mauve(\n",
    "        p_text=generated_texts, \n",
    "        q_text=reference_texts, \n",
    "        device_id=0 if cudaAvailable else -1,\n",
    "        max_text_length=256\n",
    "    )\n",
    "    return mauve_score.mauve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each key in the dictionary, create a file with the key name and write each line in the value array\n",
    "for key in diccionarioGenerado:\n",
    "    with open(f\"{key}.txt\", \"w\") as file:\n",
    "        for line in diccionarioGenerado[key]:\n",
    "            file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "for transcription in corpus['transcription']:\n",
    "    words = transcription.split()\n",
    "    unique_words.update(words)\n",
    "\n",
    "unique_words_list = sorted(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZmBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "import sentencepiece as spm\n",
    "# Path to your model checkpoint\n",
    "model_path = \"checkpoints/zmbart\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizerIsk = spm.SentencePieceProcessor(model_file=model_path+\"/spiece.model\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\").to(device)\n",
    "checkpoint = torch.load(model_path + \"/zmbart_checkpoint112.pt\")\n",
    "model_weights = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
    "model.load_state_dict(model_weights, strict=False)\n",
    "\n",
    "def safe_decode(tokenizer, output_ids):\n",
    "    decoded_tokens = []\n",
    "    \n",
    "    for token_id in output_ids:\n",
    "        #Skip 0 token\n",
    "        if token_id.item() == 0:\n",
    "            continue\n",
    "        try:\n",
    "            # Decode each token individually. Now tokenizer is a SentencePiece processor\n",
    "            decoded_token = tokenizer.decode_ids([token_id.item()])\n",
    "            \n",
    "            # If token is padding or unknown, append an empty string or placeholder\n",
    "            if decoded_token == \"<pad>\" or decoded_token == \"<unk>\" or decoded_token == \"⁇\":\n",
    "                decoded_tokens.append(\"\")\n",
    "            else:\n",
    "                decoded_tokens.append(decoded_token)\n",
    "        except IndexError:\n",
    "            # Handle index errors by appending an empty string\n",
    "            decoded_tokens.append(\"\")\n",
    "    \n",
    "    # Join tokens with spaces, replacing empty tokens appropriately\n",
    "    return ' '.join(decoded_tokens).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_samples = 1\n",
    "sentences = []\n",
    "\n",
    "while len(sentences) <= 100:\n",
    "    input_texts = random.sample(unique_words_list, num_samples)\n",
    "    # print(\"Input texts: \", input_texts)\n",
    "    \n",
    "    # Encode the input texts\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    # print(\"Encoded inputs: \", inputs)\n",
    "    \n",
    "    # Generate outputs\n",
    "    outputs = model.generate(**inputs, max_length=20, num_beams=10, early_stopping=False, no_repeat_ngram_size=1, pad_token_id=tokenizer.pad_token_id)\n",
    "    # print(\"Outputs: \", outputs)\n",
    "    \n",
    "    # Decode the outputs\n",
    "    decoded_outputs = [safe_decode(tokenizerIsk, output) for output in outputs]\n",
    "    # print(\"Decoded outputs: \", decoded_outputs)\n",
    "    sentences.extend(decoded_outputs)\n",
    "    print(\"Tenemos \", len(sentences), \" oraciones\")\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado[\"zmbart\"]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = calculate_perplexity(model, tokenizer, sentences, device)\n",
    "distinct_2 = calculate_distinct_n(sentences, n=2)\n",
    "distinct_3 = calculate_distinct_n(sentences, n=3)\n",
    "mauve_score = calculate_mauve(sentences)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Distinct-2: {distinct_2}\")\n",
    "print(f\"Distinct-3: {distinct_3}\")\n",
    "print(f\"MAUVE: {mauve_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: 43.40846252441406\n",
    "Distinct-2: 0.03824678950307091\n",
    "Distinct-3: 0.11806952025280092\n",
    "MAUVE: 0.031570415352922064"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta XNLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Path to your model checkpoint\n",
    "checkpoint_path = \"/workspace/Tesis/O3_modelos/checkpoints/metaXNLG_checkpoint-10500/\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
    "\n",
    "def safe_decode(tokenizer, output_ids):\n",
    "    # print(output_ids)\n",
    "    decoded_tokens = []\n",
    "    \n",
    "    for token_id in output_ids:\n",
    "        try:\n",
    "            # Decode each token individually\n",
    "            decoded_token = tokenizer.decode(token_id.item(), skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "            \n",
    "            # If token is padding or unknown, append an empty string or placeholder\n",
    "            if decoded_token == \"<pad>\" or decoded_token == \"<unk>\":\n",
    "                decoded_tokens.append(\"\")\n",
    "            else:\n",
    "                decoded_tokens.append(decoded_token)\n",
    "        except IndexError:\n",
    "            # Handle index errors by appending an empty string\n",
    "            decoded_tokens.append(\"\")\n",
    "    \n",
    "    # Join tokens with spaces, replacing empty tokens appropriately\n",
    "    return ' '.join(decoded_tokens).strip().replace('▁', ' ').replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a number of input texts\n",
    "num_samples = 1\n",
    "import random\n",
    "\n",
    "\n",
    "sentences = []\n",
    "\n",
    "while len(sentences) <= 100:\n",
    "    input_texts = random.sample(unique_words_list, num_samples) \n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Move inputs to GPU\n",
    "    outputs = model.generate(**inputs, max_length=30)\n",
    "    predictions = [safe_decode(tokenizer, output) for output in outputs]\n",
    "    split_sentences = [sentence.split('</s>') for sentence in predictions]\n",
    "    split_sentences = [[s.strip() for s in sentence_list if s.strip()] for sentence_list in split_sentences]\n",
    "    flattened_sentences = [item for sublist in split_sentences for item in sublist]\n",
    "    sentences.extend(flattened_sentences)\n",
    "    print(\"Tenemos \",len(sentences),\" oraciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado[\"metaXNLG\"]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas\n",
    "perplexity = calculate_perplexity(model, tokenizer, sentences, device)\n",
    "distinct_2 = calculate_distinct_n(sentences, n=2)\n",
    "distinct_3 = calculate_distinct_n(sentences, n=3)\n",
    "mauve_score = calculate_mauve(sentences)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Distinct-2: {distinct_2}\")\n",
    "print(f\"Distinct-3: {distinct_3}\")\n",
    "print(f\"MAUVE: {mauve_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: 53.81763458251953\n",
    "Distinct-2: 0.06568575932737783\n",
    "Distinct-3: 0.20421753607103219\n",
    "MAUVE: 0.5293356144783942"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define the LSTMTextGenerator class\n",
    "class LSTMTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pretrained_embeddings=None):\n",
    "        super(LSTMTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Initialize the embedding layer with pre-trained embeddings if provided\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = False  # Freeze if you don't want to fine-tune\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method initializes the hidden state and cell state for LSTM\n",
    "        num_layers = self.lstm.num_layers\n",
    "        hidden_size = self.lstm.hidden_size\n",
    "        return (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "                torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "\n",
    "# Load the vocabulary (token -> index) mapping\n",
    "vocab = {}\n",
    "with open('BaslineLSTM/tokenizadorIskonawa.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for idx, line in enumerate(vocab_file):\n",
    "        token, code = re.split(r'\\t', line.strip())\n",
    "        vocab[token] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "embed_size = 300  # Assuming the embedding size is 300, adjust if different\n",
    "vocab_size = len(vocab)\n",
    "model = LSTMTextGenerator(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = \"checkpoints/lstm/lstm_checkpoint_last.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_embeddings(embedding_file, vocab):\n",
    "#     with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "#         # Read the first line to get vocab size and embed size\n",
    "#         first_line = f.readline().strip()\n",
    "#         vocab_size, embed_size = map(int, first_line.split())\n",
    "        \n",
    "#         # Initialize a dictionary to hold the embeddings\n",
    "#         embeddings = np.zeros((len(vocab), embed_size), dtype=np.float32)\n",
    "        \n",
    "#         # Read the rest of the file\n",
    "#         for line in f:\n",
    "#             values = line.strip().split()\n",
    "#             subword = values[0].strip()\n",
    "#             vector = np.array(values[1:], dtype=np.float32)\n",
    "#             index = vocab.get(subword, -1)\n",
    "#             if index == -1:\n",
    "#                 print(f'Found {subword} in vocab')\n",
    "#             else:\n",
    "#                 embeddings[index] = vector\n",
    "    \n",
    "#     return embeddings, vocab_size, embed_size\n",
    "\n",
    "# embedding_file = 'BaslineLSTM/isk_anchor_final2.txt'\n",
    "# pretrained_embeddings, vocab_size, embed_size = load_embeddings(embedding_file, vocab)\n",
    "\n",
    "# model.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lstm(model, input_data, vocab, max_length=15):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor([[vocab[token] for token in sentence] for sentence in input_data]).to(device)\n",
    "        hidden = model.init_hidden(inputs.size(0))\n",
    "        \n",
    "        # Initialize the list to store the generated tokens\n",
    "        generated_tokens = inputs.tolist()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "            # print(predictions)\n",
    "            # Check if the predicted token is 0 and replace it with a random token if it is\n",
    "            if predictions[0, -1].item() == 0 and (random.random() < 0.4):\n",
    "                random_token = random.choice(list(vocab.values()))\n",
    "                predictions[0, -1] = random_token\n",
    "            \n",
    "            # Append the predicted token to the generated tokens\n",
    "            generated_tokens[0].append(predictions[0, -1].item())\n",
    "            \n",
    "            # Update the inputs with the predicted token\n",
    "            inputs = torch.cat((inputs, predictions[:, -1].unsqueeze(1)), dim=1)\n",
    "        \n",
    "        # Convert generated tokens back to words\n",
    "        generated_sentences = []\n",
    "        for tokens in generated_tokens:\n",
    "            sentence = [list(vocab.keys())[list(vocab.values()).index(token)] for token in tokens if token in vocab.values() and token != vocab['<unk>']]\n",
    "            generated_sentences.append(sentence)\n",
    "        \n",
    "    return generated_sentences\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    sentence=''.join(tokens)\n",
    "    return sentence.replace('▁', ' ').replace('  ', ' ').strip()\n",
    "\n",
    "# Generate sentences using the LSTM model\n",
    "num_samples = 1\n",
    "sentences = []\n",
    "reverse_vocab = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "while len(sentences) <= 100:\n",
    "    input_texts = random.sample(list(vocab.keys()), num_samples)\n",
    "    input_data = [input_texts]  # Wrap in a list to match the expected input format\n",
    "\n",
    "    # print(\"Input texts: \", input_texts)\n",
    "    predictions = test_lstm(model, input_data, vocab)\n",
    "    # print(\"Predictions: \", predictions)\n",
    " \n",
    "    flattened_sentences = [token for sentence in predictions for token in sentence]\n",
    "    # print(\"Flattened sentences: \", flattened_sentences)\n",
    "    joint_sentence=join_tokens(flattened_sentences)\n",
    "\n",
    "    sentences.extend([joint_sentence])\n",
    "    \n",
    "    # print(\"Tenemos \", len(sentences), \" oraciones\")\n",
    "    # break\n",
    "    # Break after processing one batch (for testing, remove this in real runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado[\"LSTM\"]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = calculate_perplexity_lstm(model, vocab, sentences, device)\n",
    "distinct_2 = calculate_distinct_n(sentences, n=2)\n",
    "distinct_3 = calculate_distinct_n(sentences, n=3)\n",
    "mauve_score = calculate_mauve(sentences)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Distinct-2: {distinct_2}\")\n",
    "print(f\"Distinct-3: {distinct_3}\")\n",
    "print(f\"MAUVE: {mauve_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity: nan\n",
    "Distinct-2: 0.07990182710662667\n",
    "Distinct-3: 0.31968592260235557\n",
    "MAUVE: 0.5712289309339067"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Define paths to checkpoint and SentencePiece model\n",
    "MODEL_DIR = \"checkpoints/t5\"\n",
    "TF_CHECKPOINT_PATH = \"checkpoints/t5/smallT5_model.ckpt-120000\"\n",
    "SP_MODEL_PATH = \"checkpoints/t5/spiece.model\"\n",
    "\n",
    "# Load the SentencePiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(SP_MODEL_PATH)\n",
    "\n",
    "# Tokenize the input text using the SentencePiece model\n",
    "def encode_text(text):\n",
    "    return sp.EncodeAsIds(text)\n",
    "\n",
    "# Decode output ids to text using the SentencePiece model\n",
    "def decode_text(ids):\n",
    "    return sp.DecodeIds(ids)\n",
    "\n",
    "# Load the T5 model from the TensorFlow checkpoint\n",
    "def load_model():\n",
    "    # Load the graph definition from the checkpoint meta file\n",
    "    try:\n",
    "        with gfile.GFile(f\"{TF_CHECKPOINT_PATH}.meta\", 'rb') as f:\n",
    "            graph_def = tf.compat.v1.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "    # Import the graph into the current default TensorFlow graph\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        tf.import_graph_def(graph_def, name=\"model\")\n",
    "        \n",
    "        # Restore the weights from the checkpoint\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        saver.restore(sess, TF_CHECKPOINT_PATH)\n",
    "        \n",
    "        # Return the current session\n",
    "        return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing message\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Node '': Node name contains invalid characters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/miniconda3/envs/testeo/lib/python3.10/site-packages/tensorflow/python/framework/importer.py:523\u001b[0m, in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list, propagate_device_spec)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39m_c_graph\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mas\u001b[39;00m c_graph:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_import_graphdef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_def_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m results \u001b[38;5;241m=\u001b[39m c_api_util\u001b[38;5;241m.\u001b[39mScopedTFImportGraphDefResults(results)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Node '': Node name contains invalid characters",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the model from the checkpoint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sess \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Import the graph into the current default TensorFlow graph\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_graph_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Restore the weights from the checkpoint\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     saver \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mSaver()\n",
      "File \u001b[0;32m/workspace/miniconda3/envs/testeo/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:588\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    581\u001b[0m       _log_deprecation(\n\u001b[1;32m    582\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and will \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    583\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbe removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date),\n\u001b[1;32m    587\u001b[0m           instructions)\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/miniconda3/envs/testeo/lib/python3.10/site-packages/tensorflow/python/framework/importer.py:411\u001b[0m, in \u001b[0;36mimport_graph_def\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Imports the graph from `graph_def` into the current default `Graph`.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03mThis function provides a way to import a serialized TensorFlow\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m    it refers to an unknown tensor).\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m op_dict\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_import_graph_def_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_elements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproducer_op_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproducer_op_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/miniconda3/envs/testeo/lib/python3.10/site-packages/tensorflow/python/framework/importer.py:527\u001b[0m, in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list, propagate_device_spec)\u001b[0m\n\u001b[1;32m    524\u001b[0m   results \u001b[38;5;241m=\u001b[39m c_api_util\u001b[38;5;241m.\u001b[39mScopedTFImportGraphDefResults(results)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    526\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_oss:\n",
      "\u001b[0;31mValueError\u001b[0m: Node '': Node name contains invalid characters"
     ]
    }
   ],
   "source": [
    "# Load the model from the checkpoint\n",
    "sess = load_model()\n",
    "\n",
    "# Retrieve input/output tensor names from the graph\n",
    "# input_tensor = sess.graph.get_tensor_by_name(\"input_tensor_name:0\")  # Replace with actual input tensor name\n",
    "# decoder_input_tensor = sess.graph.get_tensor_by_name(\"decoder_input_tensor_name:0\")  # Replace with actual decoder input tensor name\n",
    "# output_tensor = sess.graph.get_tensor_by_name(\"output_tensor_name:0\")  # Replace with actual output tensor name\n",
    "\n",
    "# # Run the model\n",
    "# feed_dict = {input_tensor: [input_ids], decoder_input_tensor: [decoder_input_ids]}\n",
    "# output = sess.run(output_tensor, feed_dict=feed_dict)\n",
    "\n",
    "# # Decode the output\n",
    "# decoded_output = decode_text(output[0])\n",
    "\n",
    "# print(f\"Generated text: {decoded_output}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
