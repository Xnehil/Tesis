{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación\n",
    "\n",
    "Oraciones originales de referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "corpus=pd.read_json(\"../O1_Corpus/corpus.json\", lines=True)\n",
    "\n",
    "def etapa_preprocesamiento(textos, tokenizador=None):\n",
    "    \n",
    "    #Textos es una columna de un dataframe\n",
    "    #1. Pasar a minúsculas\n",
    "    textos = textos.str.lower()\n",
    "    #2. Eliminar caracteres especiales\n",
    "    textos = textos.apply(lambda x: re.sub(r\"[\\W\\d_]+\", \" \", x))\n",
    "    textos = textos.apply(lambda x: re.sub(r\"ininteligible\", \"\", x))\n",
    "    #3. Eliminar espacios en blanco extra\n",
    "    textos = textos.apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "    #4. Eliminar espacios en blanco al principio y al final\n",
    "    textos = textos.str.strip()\n",
    "    #5. Tokenizar usando SentencePiece\n",
    "    if tokenizador:\n",
    "        textos = textos.apply(lambda x: tokenizador.encode_as_pieces(x))\n",
    "    return textos\n",
    "\n",
    "corpus['transcription'] = etapa_preprocesamiento(corpus['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_texts=corpus.sample(300)['transcription'].to_list()\n",
    "reference_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mauve\n",
    "import torch\n",
    "\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, input_texts, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    for text in input_texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(input_texts)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "def calculate_distinct_n(generated_texts, n=1):\n",
    "    n_grams = []\n",
    "    for text in generated_texts:\n",
    "        # Split the text into characters\n",
    "        chars = list(text)\n",
    "        # Generate n-grams from characters\n",
    "        n_grams.extend([tuple(chars[i:i+n]) for i in range(len(chars)-n+1)])\n",
    "\n",
    "    total_n_grams = len(n_grams)\n",
    "    unique_n_grams = len(set(n_grams))\n",
    "\n",
    "    distinct_n_score = unique_n_grams / total_n_grams if total_n_grams > 0 else 0\n",
    "    return distinct_n_score\n",
    "\n",
    "def calculate_mauve(generated_texts, reference_texts=reference_texts):\n",
    "    \"\"\"\n",
    "    Function to calculate the MAUVE score for generated texts.\n",
    "    \n",
    "    Args:\n",
    "    generated_texts (list): List of generated texts.\n",
    "    reference_texts (list): List of constant reference texts (human-written).\n",
    "    \n",
    "    Returns:\n",
    "    float: MAUVE score.\n",
    "    \"\"\"\n",
    "    cudaAvailable = torch.cuda.is_available()\n",
    "    print(f\"Using {'cuda' if cudaAvailable else 'cpu'}\")\n",
    "    mauve_score = mauve.compute_mauve(\n",
    "        p_text=generated_texts, \n",
    "        q_text=reference_texts, \n",
    "        device_id=0 if cudaAvailable else -1,\n",
    "        max_text_length=256\n",
    "    )\n",
    "    return mauve_score.mauve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "for transcription in corpus['transcription']:\n",
    "    words = transcription.split()\n",
    "    unique_words.update(words)\n",
    "\n",
    "unique_words_list = sorted(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZmBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3356/1928281794.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path + \"/zmbart_checkpoint112.pt\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MBartForConditionalGeneration:\n\tMissing key(s) in state_dict: \"final_logits_bias\", \"model.shared.weight\", \"model.encoder.embed_tokens.weight\", \"model.encoder.embed_positions.weight\", \"model.encoder.layers.0.self_attn.k_proj.weight\", \"model.encoder.layers.0.self_attn.k_proj.bias\", \"model.encoder.layers.0.self_attn.v_proj.weight\", \"model.encoder.layers.0.self_attn.v_proj.bias\", \"model.encoder.layers.0.self_attn.q_proj.weight\", \"model.encoder.layers.0.self_attn.q_proj.bias\", \"model.encoder.layers.0.self_attn.out_proj.weight\", \"model.encoder.layers.0.self_attn.out_proj.bias\", \"model.encoder.layers.0.self_attn_layer_norm.weight\", \"model.encoder.layers.0.self_attn_layer_norm.bias\", \"model.encoder.layers.0.fc1.weight\", \"model.encoder.layers.0.fc1.bias\", \"model.encoder.layers.0.fc2.weight\", \"model.encoder.layers.0.fc2.bias\", \"model.encoder.layers.0.final_layer_norm.weight\", \"model.encoder.layers.0.final_layer_norm.bias\", \"model.encoder.layers.1.self_attn.k_proj.weight\", \"model.encoder.layers.1.self_attn.k_proj.bias\", \"model.encoder.layers.1.self_attn.v_proj.weight\", \"model.encoder.layers.1.self_attn.v_proj.bias\", \"model.encoder.layers.1.self_attn.q_proj.weight\", \"model.encoder.layers.1.self_attn.q_proj.bias\", \"model.encoder.layers.1.self_attn.out_proj.weight\", \"model.encoder.layers.1.self_attn.out_proj.bias\", \"model.encoder.layers.1.self_attn_layer_norm.weight\", \"model.encoder.layers.1.self_attn_layer_norm.bias\", \"model.encoder.layers.1.fc1.weight\", \"model.encoder.layers.1.fc1.bias\", \"model.encoder.layers.1.fc2.weight\", \"model.encoder.layers.1.fc2.bias\", \"model.encoder.layers.1.final_layer_norm.weight\", \"model.encoder.layers.1.final_layer_norm.bias\", \"model.encoder.layers.2.self_attn.k_proj.weight\", \"model.encoder.layers.2.self_attn.k_proj.bias\", \"model.encoder.layers.2.self_attn.v_proj.weight\", \"model.encoder.layers.2.self_attn.v_proj.bias\", \"model.encoder.layers.2.self_attn.q_proj.weight\", \"model.encoder.layers.2.self_attn.q_proj.bias\", \"model.encoder.layers.2.self_attn.out_proj.weight\", \"model.encoder.layers.2.self_attn.out_proj.bias\", \"model.encoder.layers.2.self_attn_layer_norm.weight\", \"model.encoder.layers.2.self_attn_layer_norm.bias\", \"model.encoder.layers.2.fc1.weight\", \"model.encoder.layers.2.fc1.bias\", \"model.encoder.layers.2.fc2.weight\", \"model.encoder.layers.2.fc2.bias\", \"model.encoder.layers.2.final_layer_norm.weight\", \"model.encoder.layers.2.final_layer_norm.bias\", \"model.encoder.layers.3.self_attn.k_proj.weight\", \"model.encoder.layers.3.self_attn.k_proj.bias\", \"model.encoder.layers.3.self_attn.v_proj.weight\", \"model.encoder.layers.3.self_attn.v_proj.bias\", \"model.encoder.layers.3.self_attn.q_proj.weight\", \"model.encoder.layers.3.self_attn.q_proj.bias\", \"model.encoder.layers.3.self_attn.out_proj.weight\", \"model.encoder.layers.3.self_attn.out_proj.bias\", \"model.encoder.layers.3.self_attn_layer_norm.weight\", \"model.encoder.layers.3.self_attn_layer_norm.bias\", \"model.encoder.layers.3.fc1.weight\", \"model.encoder.layers.3.fc1.bias\", \"model.encoder.layers.3.fc2.weight\", \"model.encoder.layers.3.fc2.bias\", \"model.encoder.layers.3.final_layer_norm.weight\", \"model.encoder.layers.3.final_layer_norm.bias\", \"model.encoder.layers.4.self_attn.k_proj.weight\", \"model.encoder.layers.4.self_attn.k_proj.bias\", \"model.encoder.layers.4.self_attn.v_proj.weight\", \"model.encoder.layers.4.self_attn.v_proj.bias\", \"model.encoder.layers.4.self_attn.q_proj.weight\", \"model.encoder.layers.4.self_attn.q_proj.bias\", \"model.encoder.layers.4.self_attn.out_proj.weight\", \"model.encoder.layers.4.self_attn.out_proj.bias\", \"model.encoder.layers.4.self_attn_layer_norm.weight\", \"model.encoder.layers.4.self_attn_layer_norm.bias\", \"model.encoder.layers.4.fc1.weight\", \"model.encoder.layers.4.fc1.bias\", \"model.encoder.layers.4.fc2.weight\", \"model.encoder.layers.4.fc2.bias\", \"model.encoder.layers.4.final_layer_norm.weight\", \"model.encoder.layers.4.final_layer_norm.bias\", \"model.encoder.layers.5.self_attn.k_proj.weight\", \"model.encoder.layers.5.self_attn.k_proj.bias\", \"model.encoder.layers.5.self_attn.v_proj.weight\", \"model.encoder.layers.5.self_attn.v_proj.bias\", \"model.encoder.layers.5.self_attn.q_proj.weight\", \"model.encoder.layers.5.self_attn.q_proj.bias\", \"model.encoder.layers.5.self_attn.out_proj.weight\", \"model.encoder.layers.5.self_attn.out_proj.bias\", \"model.encoder.layers.5.self_attn_layer_norm.weight\", \"model.encoder.layers.5.self_attn_layer_norm.bias\", \"model.encoder.layers.5.fc1.weight\", \"model.encoder.layers.5.fc1.bias\", \"model.encoder.layers.5.fc2.weight\", \"model.encoder.layers.5.fc2.bias\", \"model.encoder.layers.5.final_layer_norm.weight\", \"model.encoder.layers.5.final_layer_norm.bias\", \"model.encoder.layers.6.self_attn.k_proj.weight\", \"model.encoder.layers.6.self_attn.k_proj.bias\", \"model.encoder.layers.6.self_attn.v_proj.weight\", \"model.encoder.layers.6.self_attn.v_proj.bias\", \"model.encoder.layers.6.self_attn.q_proj.weight\", \"model.encoder.layers.6.self_attn.q_proj.bias\", \"model.encoder.layers.6.self_attn.out_proj.weight\", \"model.encoder.layers.6.self_attn.out_proj.bias\", \"model.encoder.layers.6.self_attn_layer_norm.weight\", \"model.encoder.layers.6.self_attn_layer_norm.bias\", \"model.encoder.layers.6.fc1.weight\", \"model.encoder.layers.6.fc1.bias\", \"model.encoder.layers.6.fc2.weight\", \"model.encoder.layers.6.fc2.bias\", \"model.encoder.layers.6.final_layer_norm.weight\", \"model.encoder.layers.6.final_layer_norm.bias\", \"model.encoder.layers.7.self_attn.k_proj.weight\", \"model.encoder.layers.7.self_attn.k_proj.bias\", \"model.encoder.layers.7.self_attn.v_proj.weight\", \"model.encoder.layers.7.self_attn.v_proj.bias\", \"model.encoder.layers.7.self_attn.q_proj.weight\", \"model.encoder.layers.7.self_attn.q_proj.bias\", \"model.encoder.layers.7.self_attn.out_proj.weight\", \"model.encoder.layers.7.self_attn.out_proj.bias\", \"model.encoder.layers.7.self_attn_layer_norm.weight\", \"model.encoder.layers.7.self_attn_layer_norm.bias\", \"model.encoder.layers.7.fc1.weight\", \"model.encoder.layers.7.fc1.bias\", \"model.encoder.layers.7.fc2.weight\", \"model.encoder.layers.7.fc2.bias\", \"model.encoder.layers.7.final_layer_norm.weight\", \"model.encoder.layers.7.final_layer_norm.bias\", \"model.encoder.layers.8.self_attn.k_proj.weight\", \"model.encoder.layers.8.self_attn.k_proj.bias\", \"model.encoder.layers.8.self_attn.v_proj.weight\", \"model.encoder.layers.8.self_attn.v_proj.bias\", \"model.encoder.layers.8.self_attn.q_proj.weight\", \"model.encoder.layers.8.self_attn.q_proj.bias\", \"model.encoder.layers.8.self_attn.out_proj.weight\", \"model.encoder.layers.8.self_attn.out_proj.bias\", \"model.encoder.layers.8.self_attn_layer_norm.weight\", \"model.encoder.layers.8.self_attn_layer_norm.bias\", \"model.encoder.layers.8.fc1.weight\", \"model.encoder.layers.8.fc1.bias\", \"model.encoder.layers.8.fc2.weight\", \"model.encoder.layers.8.fc2.bias\", \"model.encoder.layers.8.final_layer_norm.weight\", \"model.encoder.layers.8.final_layer_norm.bias\", \"model.encoder.layers.9.self_attn.k_proj.weight\", \"model.encoder.layers.9.self_attn.k_proj.bias\", \"model.encoder.layers.9.self_attn.v_proj.weight\", \"model.encoder.layers.9.self_attn.v_proj.bias\", \"model.encoder.layers.9.self_attn.q_proj.weight\", \"model.encoder.layers.9.self_attn.q_proj.bias\", \"model.encoder.layers.9.self_attn.out_proj.weight\", \"model.encoder.layers.9.self_attn.out_proj.bias\", \"model.encoder.layers.9.self_attn_layer_norm.weight\", \"model.encoder.layers.9.self_attn_layer_norm.bias\", \"model.encoder.layers.9.fc1.weight\", \"model.encoder.layers.9.fc1.bias\", \"model.encoder.layers.9.fc2.weight\", \"model.encoder.layers.9.fc2.bias\", \"model.encoder.layers.9.final_layer_norm.weight\", \"model.encoder.layers.9.final_layer_norm.bias\", \"model.encoder.layers.10.self_attn.k_proj.weight\", \"model.encoder.layers.10.self_attn.k_proj.bias\", \"model.encoder.layers.10.self_attn.v_proj.weight\", \"model.encoder.layers.10.self_attn.v_proj.bias\", \"model.encoder.layers.10.self_attn.q_proj.weight\", \"model.encoder.layers.10.self_attn.q_proj.bias\", \"model.encoder.layers.10.self_attn.out_proj.weight\", \"model.encoder.layers.10.self_attn.out_proj.bias\", \"model.encoder.layers.10.self_attn_layer_norm.weight\", \"model.encoder.layers.10.self_attn_layer_norm.bias\", \"model.encoder.layers.10.fc1.weight\", \"model.encoder.layers.10.fc1.bias\", \"model.encoder.layers.10.fc2.weight\", \"model.encoder.layers.10.fc2.bias\", \"model.encoder.layers.10.final_layer_norm.weight\", \"model.encoder.layers.10.final_layer_norm.bias\", \"model.encoder.layers.11.self_attn.k_proj.weight\", \"model.encoder.layers.11.self_attn.k_proj.bias\", \"model.encoder.layers.11.self_attn.v_proj.weight\", \"model.encoder.layers.11.self_attn.v_proj.bias\", \"model.encoder.layers.11.self_attn.q_proj.weight\", \"model.encoder.layers.11.self_attn.q_proj.bias\", \"model.encoder.layers.11.self_attn.out_proj.weight\", \"model.encoder.layers.11.self_attn.out_proj.bias\", \"model.encoder.layers.11.self_attn_layer_norm.weight\", \"model.encoder.layers.11.self_attn_layer_norm.bias\", \"model.encoder.layers.11.fc1.weight\", \"model.encoder.layers.11.fc1.bias\", \"model.encoder.layers.11.fc2.weight\", \"model.encoder.layers.11.fc2.bias\", \"model.encoder.layers.11.final_layer_norm.weight\", \"model.encoder.layers.11.final_layer_norm.bias\", \"model.encoder.layernorm_embedding.weight\", \"model.encoder.layernorm_embedding.bias\", \"model.encoder.layer_norm.weight\", \"model.encoder.layer_norm.bias\", \"model.decoder.embed_tokens.weight\", \"model.decoder.embed_positions.weight\", \"model.decoder.layers.0.self_attn.k_proj.weight\", \"model.decoder.layers.0.self_attn.k_proj.bias\", \"model.decoder.layers.0.self_attn.v_proj.weight\", \"model.decoder.layers.0.self_attn.v_proj.bias\", \"model.decoder.layers.0.self_attn.q_proj.weight\", \"model.decoder.layers.0.self_attn.q_proj.bias\", \"model.decoder.layers.0.self_attn.out_proj.weight\", \"model.decoder.layers.0.self_attn.out_proj.bias\", \"model.decoder.layers.0.self_attn_layer_norm.weight\", \"model.decoder.layers.0.self_attn_layer_norm.bias\", \"model.decoder.layers.0.encoder_attn.k_proj.weight\", \"model.decoder.layers.0.encoder_attn.k_proj.bias\", \"model.decoder.layers.0.encoder_attn.v_proj.weight\", \"model.decoder.layers.0.encoder_attn.v_proj.bias\", \"model.decoder.layers.0.encoder_attn.q_proj.weight\", \"model.decoder.layers.0.encoder_attn.q_proj.bias\", \"model.decoder.layers.0.encoder_attn.out_proj.weight\", \"model.decoder.layers.0.encoder_attn.out_proj.bias\", \"model.decoder.layers.0.encoder_attn_layer_norm.weight\", \"model.decoder.layers.0.encoder_attn_layer_norm.bias\", \"model.decoder.layers.0.fc1.weight\", \"model.decoder.layers.0.fc1.bias\", \"model.decoder.layers.0.fc2.weight\", \"model.decoder.layers.0.fc2.bias\", \"model.decoder.layers.0.final_layer_norm.weight\", \"model.decoder.layers.0.final_layer_norm.bias\", \"model.decoder.layers.1.self_attn.k_proj.weight\", \"model.decoder.layers.1.self_attn.k_proj.bias\", \"model.decoder.layers.1.self_attn.v_proj.weight\", \"model.decoder.layers.1.self_attn.v_proj.bias\", \"model.decoder.layers.1.self_attn.q_proj.weight\", \"model.decoder.layers.1.self_attn.q_proj.bias\", \"model.decoder.layers.1.self_attn.out_proj.weight\", \"model.decoder.layers.1.self_attn.out_proj.bias\", \"model.decoder.layers.1.self_attn_layer_norm.weight\", \"model.decoder.layers.1.self_attn_layer_norm.bias\", \"model.decoder.layers.1.encoder_attn.k_proj.weight\", \"model.decoder.layers.1.encoder_attn.k_proj.bias\", \"model.decoder.layers.1.encoder_attn.v_proj.weight\", \"model.decoder.layers.1.encoder_attn.v_proj.bias\", \"model.decoder.layers.1.encoder_attn.q_proj.weight\", \"model.decoder.layers.1.encoder_attn.q_proj.bias\", \"model.decoder.layers.1.encoder_attn.out_proj.weight\", \"model.decoder.layers.1.encoder_attn.out_proj.bias\", \"model.decoder.layers.1.encoder_attn_layer_norm.weight\", \"model.decoder.layers.1.encoder_attn_layer_norm.bias\", \"model.decoder.layers.1.fc1.weight\", \"model.decoder.layers.1.fc1.bias\", \"model.decoder.layers.1.fc2.weight\", \"model.decoder.layers.1.fc2.bias\", \"model.decoder.layers.1.final_layer_norm.weight\", \"model.decoder.layers.1.final_layer_norm.bias\", \"model.decoder.layers.2.self_attn.k_proj.weight\", \"model.decoder.layers.2.self_attn.k_proj.bias\", \"model.decoder.layers.2.self_attn.v_proj.weight\", \"model.decoder.layers.2.self_attn.v_proj.bias\", \"model.decoder.layers.2.self_attn.q_proj.weight\", \"model.decoder.layers.2.self_attn.q_proj.bias\", \"model.decoder.layers.2.self_attn.out_proj.weight\", \"model.decoder.layers.2.self_attn.out_proj.bias\", \"model.decoder.layers.2.self_attn_layer_norm.weight\", \"model.decoder.layers.2.self_attn_layer_norm.bias\", \"model.decoder.layers.2.encoder_attn.k_proj.weight\", \"model.decoder.layers.2.encoder_attn.k_proj.bias\", \"model.decoder.layers.2.encoder_attn.v_proj.weight\", \"model.decoder.layers.2.encoder_attn.v_proj.bias\", \"model.decoder.layers.2.encoder_attn.q_proj.weight\", \"model.decoder.layers.2.encoder_attn.q_proj.bias\", \"model.decoder.layers.2.encoder_attn.out_proj.weight\", \"model.decoder.layers.2.encoder_attn.out_proj.bias\", \"model.decoder.layers.2.encoder_attn_layer_norm.weight\", \"model.decoder.layers.2.encoder_attn_layer_norm.bias\", \"model.decoder.layers.2.fc1.weight\", \"model.decoder.layers.2.fc1.bias\", \"model.decoder.layers.2.fc2.weight\", \"model.decoder.layers.2.fc2.bias\", \"model.decoder.layers.2.final_layer_norm.weight\", \"model.decoder.layers.2.final_layer_norm.bias\", \"model.decoder.layers.3.self_attn.k_proj.weight\", \"model.decoder.layers.3.self_attn.k_proj.bias\", \"model.decoder.layers.3.self_attn.v_proj.weight\", \"model.decoder.layers.3.self_attn.v_proj.bias\", \"model.decoder.layers.3.self_attn.q_proj.weight\", \"model.decoder.layers.3.self_attn.q_proj.bias\", \"model.decoder.layers.3.self_attn.out_proj.weight\", \"model.decoder.layers.3.self_attn.out_proj.bias\", \"model.decoder.layers.3.self_attn_layer_norm.weight\", \"model.decoder.layers.3.self_attn_layer_norm.bias\", \"model.decoder.layers.3.encoder_attn.k_proj.weight\", \"model.decoder.layers.3.encoder_attn.k_proj.bias\", \"model.decoder.layers.3.encoder_attn.v_proj.weight\", \"model.decoder.layers.3.encoder_attn.v_proj.bias\", \"model.decoder.layers.3.encoder_attn.q_proj.weight\", \"model.decoder.layers.3.encoder_attn.q_proj.bias\", \"model.decoder.layers.3.encoder_attn.out_proj.weight\", \"model.decoder.layers.3.encoder_attn.out_proj.bias\", \"model.decoder.layers.3.encoder_attn_layer_norm.weight\", \"model.decoder.layers.3.encoder_attn_layer_norm.bias\", \"model.decoder.layers.3.fc1.weight\", \"model.decoder.layers.3.fc1.bias\", \"model.decoder.layers.3.fc2.weight\", \"model.decoder.layers.3.fc2.bias\", \"model.decoder.layers.3.final_layer_norm.weight\", \"model.decoder.layers.3.final_layer_norm.bias\", \"model.decoder.layers.4.self_attn.k_proj.weight\", \"model.decoder.layers.4.self_attn.k_proj.bias\", \"model.decoder.layers.4.self_attn.v_proj.weight\", \"model.decoder.layers.4.self_attn.v_proj.bias\", \"model.decoder.layers.4.self_attn.q_proj.weight\", \"model.decoder.layers.4.self_attn.q_proj.bias\", \"model.decoder.layers.4.self_attn.out_proj.weight\", \"model.decoder.layers.4.self_attn.out_proj.bias\", \"model.decoder.layers.4.self_attn_layer_norm.weight\", \"model.decoder.layers.4.self_attn_layer_norm.bias\", \"model.decoder.layers.4.encoder_attn.k_proj.weight\", \"model.decoder.layers.4.encoder_attn.k_proj.bias\", \"model.decoder.layers.4.encoder_attn.v_proj.weight\", \"model.decoder.layers.4.encoder_attn.v_proj.bias\", \"model.decoder.layers.4.encoder_attn.q_proj.weight\", \"model.decoder.layers.4.encoder_attn.q_proj.bias\", \"model.decoder.layers.4.encoder_attn.out_proj.weight\", \"model.decoder.layers.4.encoder_attn.out_proj.bias\", \"model.decoder.layers.4.encoder_attn_layer_norm.weight\", \"model.decoder.layers.4.encoder_attn_layer_norm.bias\", \"model.decoder.layers.4.fc1.weight\", \"model.decoder.layers.4.fc1.bias\", \"model.decoder.layers.4.fc2.weight\", \"model.decoder.layers.4.fc2.bias\", \"model.decoder.layers.4.final_layer_norm.weight\", \"model.decoder.layers.4.final_layer_norm.bias\", \"model.decoder.layers.5.self_attn.k_proj.weight\", \"model.decoder.layers.5.self_attn.k_proj.bias\", \"model.decoder.layers.5.self_attn.v_proj.weight\", \"model.decoder.layers.5.self_attn.v_proj.bias\", \"model.decoder.layers.5.self_attn.q_proj.weight\", \"model.decoder.layers.5.self_attn.q_proj.bias\", \"model.decoder.layers.5.self_attn.out_proj.weight\", \"model.decoder.layers.5.self_attn.out_proj.bias\", \"model.decoder.layers.5.self_attn_layer_norm.weight\", \"model.decoder.layers.5.self_attn_layer_norm.bias\", \"model.decoder.layers.5.encoder_attn.k_proj.weight\", \"model.decoder.layers.5.encoder_attn.k_proj.bias\", \"model.decoder.layers.5.encoder_attn.v_proj.weight\", \"model.decoder.layers.5.encoder_attn.v_proj.bias\", \"model.decoder.layers.5.encoder_attn.q_proj.weight\", \"model.decoder.layers.5.encoder_attn.q_proj.bias\", \"model.decoder.layers.5.encoder_attn.out_proj.weight\", \"model.decoder.layers.5.encoder_attn.out_proj.bias\", \"model.decoder.layers.5.encoder_attn_layer_norm.weight\", \"model.decoder.layers.5.encoder_attn_layer_norm.bias\", \"model.decoder.layers.5.fc1.weight\", \"model.decoder.layers.5.fc1.bias\", \"model.decoder.layers.5.fc2.weight\", \"model.decoder.layers.5.fc2.bias\", \"model.decoder.layers.5.final_layer_norm.weight\", \"model.decoder.layers.5.final_layer_norm.bias\", \"model.decoder.layers.6.self_attn.k_proj.weight\", \"model.decoder.layers.6.self_attn.k_proj.bias\", \"model.decoder.layers.6.self_attn.v_proj.weight\", \"model.decoder.layers.6.self_attn.v_proj.bias\", \"model.decoder.layers.6.self_attn.q_proj.weight\", \"model.decoder.layers.6.self_attn.q_proj.bias\", \"model.decoder.layers.6.self_attn.out_proj.weight\", \"model.decoder.layers.6.self_attn.out_proj.bias\", \"model.decoder.layers.6.self_attn_layer_norm.weight\", \"model.decoder.layers.6.self_attn_layer_norm.bias\", \"model.decoder.layers.6.encoder_attn.k_proj.weight\", \"model.decoder.layers.6.encoder_attn.k_proj.bias\", \"model.decoder.layers.6.encoder_attn.v_proj.weight\", \"model.decoder.layers.6.encoder_attn.v_proj.bias\", \"model.decoder.layers.6.encoder_attn.q_proj.weight\", \"model.decoder.layers.6.encoder_attn.q_proj.bias\", \"model.decoder.layers.6.encoder_attn.out_proj.weight\", \"model.decoder.layers.6.encoder_attn.out_proj.bias\", \"model.decoder.layers.6.encoder_attn_layer_norm.weight\", \"model.decoder.layers.6.encoder_attn_layer_norm.bias\", \"model.decoder.layers.6.fc1.weight\", \"model.decoder.layers.6.fc1.bias\", \"model.decoder.layers.6.fc2.weight\", \"model.decoder.layers.6.fc2.bias\", \"model.decoder.layers.6.final_layer_norm.weight\", \"model.decoder.layers.6.final_layer_norm.bias\", \"model.decoder.layers.7.self_attn.k_proj.weight\", \"model.decoder.layers.7.self_attn.k_proj.bias\", \"model.decoder.layers.7.self_attn.v_proj.weight\", \"model.decoder.layers.7.self_attn.v_proj.bias\", \"model.decoder.layers.7.self_attn.q_proj.weight\", \"model.decoder.layers.7.self_attn.q_proj.bias\", \"model.decoder.layers.7.self_attn.out_proj.weight\", \"model.decoder.layers.7.self_attn.out_proj.bias\", \"model.decoder.layers.7.self_attn_layer_norm.weight\", \"model.decoder.layers.7.self_attn_layer_norm.bias\", \"model.decoder.layers.7.encoder_attn.k_proj.weight\", \"model.decoder.layers.7.encoder_attn.k_proj.bias\", \"model.decoder.layers.7.encoder_attn.v_proj.weight\", \"model.decoder.layers.7.encoder_attn.v_proj.bias\", \"model.decoder.layers.7.encoder_attn.q_proj.weight\", \"model.decoder.layers.7.encoder_attn.q_proj.bias\", \"model.decoder.layers.7.encoder_attn.out_proj.weight\", \"model.decoder.layers.7.encoder_attn.out_proj.bias\", \"model.decoder.layers.7.encoder_attn_layer_norm.weight\", \"model.decoder.layers.7.encoder_attn_layer_norm.bias\", \"model.decoder.layers.7.fc1.weight\", \"model.decoder.layers.7.fc1.bias\", \"model.decoder.layers.7.fc2.weight\", \"model.decoder.layers.7.fc2.bias\", \"model.decoder.layers.7.final_layer_norm.weight\", \"model.decoder.layers.7.final_layer_norm.bias\", \"model.decoder.layers.8.self_attn.k_proj.weight\", \"model.decoder.layers.8.self_attn.k_proj.bias\", \"model.decoder.layers.8.self_attn.v_proj.weight\", \"model.decoder.layers.8.self_attn.v_proj.bias\", \"model.decoder.layers.8.self_attn.q_proj.weight\", \"model.decoder.layers.8.self_attn.q_proj.bias\", \"model.decoder.layers.8.self_attn.out_proj.weight\", \"model.decoder.layers.8.self_attn.out_proj.bias\", \"model.decoder.layers.8.self_attn_layer_norm.weight\", \"model.decoder.layers.8.self_attn_layer_norm.bias\", \"model.decoder.layers.8.encoder_attn.k_proj.weight\", \"model.decoder.layers.8.encoder_attn.k_proj.bias\", \"model.decoder.layers.8.encoder_attn.v_proj.weight\", \"model.decoder.layers.8.encoder_attn.v_proj.bias\", \"model.decoder.layers.8.encoder_attn.q_proj.weight\", \"model.decoder.layers.8.encoder_attn.q_proj.bias\", \"model.decoder.layers.8.encoder_attn.out_proj.weight\", \"model.decoder.layers.8.encoder_attn.out_proj.bias\", \"model.decoder.layers.8.encoder_attn_layer_norm.weight\", \"model.decoder.layers.8.encoder_attn_layer_norm.bias\", \"model.decoder.layers.8.fc1.weight\", \"model.decoder.layers.8.fc1.bias\", \"model.decoder.layers.8.fc2.weight\", \"model.decoder.layers.8.fc2.bias\", \"model.decoder.layers.8.final_layer_norm.weight\", \"model.decoder.layers.8.final_layer_norm.bias\", \"model.decoder.layers.9.self_attn.k_proj.weight\", \"model.decoder.layers.9.self_attn.k_proj.bias\", \"model.decoder.layers.9.self_attn.v_proj.weight\", \"model.decoder.layers.9.self_attn.v_proj.bias\", \"model.decoder.layers.9.self_attn.q_proj.weight\", \"model.decoder.layers.9.self_attn.q_proj.bias\", \"model.decoder.layers.9.self_attn.out_proj.weight\", \"model.decoder.layers.9.self_attn.out_proj.bias\", \"model.decoder.layers.9.self_attn_layer_norm.weight\", \"model.decoder.layers.9.self_attn_layer_norm.bias\", \"model.decoder.layers.9.encoder_attn.k_proj.weight\", \"model.decoder.layers.9.encoder_attn.k_proj.bias\", \"model.decoder.layers.9.encoder_attn.v_proj.weight\", \"model.decoder.layers.9.encoder_attn.v_proj.bias\", \"model.decoder.layers.9.encoder_attn.q_proj.weight\", \"model.decoder.layers.9.encoder_attn.q_proj.bias\", \"model.decoder.layers.9.encoder_attn.out_proj.weight\", \"model.decoder.layers.9.encoder_attn.out_proj.bias\", \"model.decoder.layers.9.encoder_attn_layer_norm.weight\", \"model.decoder.layers.9.encoder_attn_layer_norm.bias\", \"model.decoder.layers.9.fc1.weight\", \"model.decoder.layers.9.fc1.bias\", \"model.decoder.layers.9.fc2.weight\", \"model.decoder.layers.9.fc2.bias\", \"model.decoder.layers.9.final_layer_norm.weight\", \"model.decoder.layers.9.final_layer_norm.bias\", \"model.decoder.layers.10.self_attn.k_proj.weight\", \"model.decoder.layers.10.self_attn.k_proj.bias\", \"model.decoder.layers.10.self_attn.v_proj.weight\", \"model.decoder.layers.10.self_attn.v_proj.bias\", \"model.decoder.layers.10.self_attn.q_proj.weight\", \"model.decoder.layers.10.self_attn.q_proj.bias\", \"model.decoder.layers.10.self_attn.out_proj.weight\", \"model.decoder.layers.10.self_attn.out_proj.bias\", \"model.decoder.layers.10.self_attn_layer_norm.weight\", \"model.decoder.layers.10.self_attn_layer_norm.bias\", \"model.decoder.layers.10.encoder_attn.k_proj.weight\", \"model.decoder.layers.10.encoder_attn.k_proj.bias\", \"model.decoder.layers.10.encoder_attn.v_proj.weight\", \"model.decoder.layers.10.encoder_attn.v_proj.bias\", \"model.decoder.layers.10.encoder_attn.q_proj.weight\", \"model.decoder.layers.10.encoder_attn.q_proj.bias\", \"model.decoder.layers.10.encoder_attn.out_proj.weight\", \"model.decoder.layers.10.encoder_attn.out_proj.bias\", \"model.decoder.layers.10.encoder_attn_layer_norm.weight\", \"model.decoder.layers.10.encoder_attn_layer_norm.bias\", \"model.decoder.layers.10.fc1.weight\", \"model.decoder.layers.10.fc1.bias\", \"model.decoder.layers.10.fc2.weight\", \"model.decoder.layers.10.fc2.bias\", \"model.decoder.layers.10.final_layer_norm.weight\", \"model.decoder.layers.10.final_layer_norm.bias\", \"model.decoder.layers.11.self_attn.k_proj.weight\", \"model.decoder.layers.11.self_attn.k_proj.bias\", \"model.decoder.layers.11.self_attn.v_proj.weight\", \"model.decoder.layers.11.self_attn.v_proj.bias\", \"model.decoder.layers.11.self_attn.q_proj.weight\", \"model.decoder.layers.11.self_attn.q_proj.bias\", \"model.decoder.layers.11.self_attn.out_proj.weight\", \"model.decoder.layers.11.self_attn.out_proj.bias\", \"model.decoder.layers.11.self_attn_layer_norm.weight\", \"model.decoder.layers.11.self_attn_layer_norm.bias\", \"model.decoder.layers.11.encoder_attn.k_proj.weight\", \"model.decoder.layers.11.encoder_attn.k_proj.bias\", \"model.decoder.layers.11.encoder_attn.v_proj.weight\", \"model.decoder.layers.11.encoder_attn.v_proj.bias\", \"model.decoder.layers.11.encoder_attn.q_proj.weight\", \"model.decoder.layers.11.encoder_attn.q_proj.bias\", \"model.decoder.layers.11.encoder_attn.out_proj.weight\", \"model.decoder.layers.11.encoder_attn.out_proj.bias\", \"model.decoder.layers.11.encoder_attn_layer_norm.weight\", \"model.decoder.layers.11.encoder_attn_layer_norm.bias\", \"model.decoder.layers.11.fc1.weight\", \"model.decoder.layers.11.fc1.bias\", \"model.decoder.layers.11.fc2.weight\", \"model.decoder.layers.11.fc2.bias\", \"model.decoder.layers.11.final_layer_norm.weight\", \"model.decoder.layers.11.final_layer_norm.bias\", \"model.decoder.layernorm_embedding.weight\", \"model.decoder.layernorm_embedding.bias\", \"model.decoder.layer_norm.weight\", \"model.decoder.layer_norm.bias\", \"lm_head.weight\". \n\tUnexpected key(s) in state_dict: \"args\", \"model\", \"optimizer_history\", \"extra_state\", \"last_optimizer_state\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m MBartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/mbart-large-cc25\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned weights\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/zmbart_checkpoint112.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_decode\u001b[39m(tokenizer, output_ids):\n\u001b[1;32m     22\u001b[0m     decoded_tokens \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/workspace/miniconda3/envs/testeo/lib/python3.10/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MBartForConditionalGeneration:\n\tMissing key(s) in state_dict: \"final_logits_bias\", \"model.shared.weight\", \"model.encoder.embed_tokens.weight\", \"model.encoder.embed_positions.weight\", \"model.encoder.layers.0.self_attn.k_proj.weight\", \"model.encoder.layers.0.self_attn.k_proj.bias\", \"model.encoder.layers.0.self_attn.v_proj.weight\", \"model.encoder.layers.0.self_attn.v_proj.bias\", \"model.encoder.layers.0.self_attn.q_proj.weight\", \"model.encoder.layers.0.self_attn.q_proj.bias\", \"model.encoder.layers.0.self_attn.out_proj.weight\", \"model.encoder.layers.0.self_attn.out_proj.bias\", \"model.encoder.layers.0.self_attn_layer_norm.weight\", \"model.encoder.layers.0.self_attn_layer_norm.bias\", \"model.encoder.layers.0.fc1.weight\", \"model.encoder.layers.0.fc1.bias\", \"model.encoder.layers.0.fc2.weight\", \"model.encoder.layers.0.fc2.bias\", \"model.encoder.layers.0.final_layer_norm.weight\", \"model.encoder.layers.0.final_layer_norm.bias\", \"model.encoder.layers.1.self_attn.k_proj.weight\", \"model.encoder.layers.1.self_attn.k_proj.bias\", \"model.encoder.layers.1.self_attn.v_proj.weight\", \"model.encoder.layers.1.self_attn.v_proj.bias\", \"model.encoder.layers.1.self_attn.q_proj.weight\", \"model.encoder.layers.1.self_attn.q_proj.bias\", \"model.encoder.layers.1.self_attn.out_proj.weight\", \"model.encoder.layers.1.self_attn.out_proj.bias\", \"model.encoder.layers.1.self_attn_layer_norm.weight\", \"model.encoder.layers.1.self_attn_layer_norm.bias\", \"model.encoder.layers.1.fc1.weight\", \"model.encoder.layers.1.fc1.bias\", \"model.encoder.layers.1.fc2.weight\", \"model.encoder.layers.1.fc2.bias\", \"model.encoder.layers.1.final_layer_norm.weight\", \"model.encoder.layers.1.final_layer_norm.bias\", \"model.encoder.layers.2.self_attn.k_proj.weight\", \"model.encoder.layers.2.self_attn.k_proj.bias\", \"model.encoder.layers.2.self_attn.v_proj.weight\", \"model.encoder.layers.2.self_attn.v_proj.bias\", \"model.encoder.layers.2.self_attn.q_proj.weight\", \"model.encoder.layers.2.self_attn.q_proj.bias\", \"model.encoder.layers.2.self_attn.out_proj.weight\", \"model.encoder.layers.2.self_attn.out_proj.bias\", \"model.encoder.layers.2.self_attn_layer_norm.weight\", \"model.encoder.layers.2.self_attn_layer_norm.bias\", \"model.encoder.layers.2.fc1.weight\", \"model.encoder.layers.2.fc1.bias\", \"model.encoder.layers.2.fc2.weight\", \"model.encoder.layers.2.fc2.bias\", \"model.encoder.layers.2.final_layer_norm.weight\", \"model.encoder.layers.2.final_layer_norm.bias\", \"model.encoder.layers.3.self_attn.k_proj.weight\", \"model.encoder.layers.3.self_attn.k_proj.bias\", \"model.encoder.layers.3.self_attn.v_proj.weight\", \"model.encoder.layers.3.self_attn.v_proj.bias\", \"model.encoder.layers.3.self_attn.q_proj.weight\", \"model.encoder.layers.3.self_attn.q_proj.bias\", \"model.encoder.layers.3.self_attn.out_proj.weight\", \"model.encoder.layers.3.self_attn.out_proj.bias\", \"model.encoder.layers.3.self_attn_layer_norm.weight\", \"model.encoder.layers.3.self_attn_layer_norm.bias\", \"model.encoder.layers.3.fc1.weight\", \"model.encoder.layers.3.fc1.bias\", \"model.encoder.layers.3.fc2.weight\", \"model.encoder.layers.3.fc2.bias\", \"model.encoder.layers.3.final_layer_norm.weight\", \"model.encoder.layers.3.final_layer_norm.bias\", \"model.encoder.layers.4.self_attn.k_proj.weight\", \"model.encoder.layers.4.self_attn.k_proj.bias\", \"model.encoder.layers.4.self_attn.v_proj.weight\", \"model.encoder.layers.4.self_attn.v_proj.bias\", \"model.encoder.layers.4.self_attn.q_proj.weight\", \"model.encoder.layers.4.self_attn.q_proj.bias\", \"model.encoder.layers.4.self_attn.out_proj.weight\", \"model.encoder.layers.4.self_attn.out_proj.bias\", \"model.encoder.layers.4.self_attn_layer_norm.weight\", \"model.encoder.layers.4.self_attn_layer_norm.bias\", \"model.encoder.layers.4.fc1.weight\", \"model.encoder.layers.4.fc1.bias\", \"model.encoder.layers.4.fc2.weight\", \"model.encoder.layers.4.fc2.bias\", \"model.encoder.layers.4.final_layer_norm.weight\", \"model.encoder.layers.4.final_layer_norm.bias\", \"model.encoder.layers.5.self_attn.k_proj.weight\", \"model.encoder.layers.5.self_attn.k_proj.bias\", \"model.encoder.layers.5.self_attn.v_proj.weight\", \"model.encoder.layers.5.self_attn.v_proj.bias\", \"model.encoder.layers.5.self_attn.q_proj.weight\", \"model.encoder.layers.5.self_attn.q_proj.bias\", \"model.encoder.layers.5.self_attn.out_proj.weight\", \"model.encoder.layers.5.self_attn.out_proj.bias\", \"model.encoder.layers.5.self_attn_layer_norm.weight\", \"model.encoder.layers.5.self_attn_layer_norm.bias\", \"model.encoder.layers.5.fc1.weight\", \"model.encoder.layers.5.fc1.bias\", \"model.encoder.layers.5.fc2.weight\", \"model.encoder.layers.5.fc2.bias\", \"model.encoder.layers.5.final_layer_norm.weight\", \"model.encoder.layers.5.final_layer_norm.bias\", \"model.encoder.layers.6.self_attn.k_proj.weight\", \"model.encoder.layers.6.self_attn.k_proj.bias\", \"model.encoder.layers.6.self_attn.v_proj.weight\", \"model.encoder.layers.6.self_attn.v_proj.bias\", \"model.encoder.layers.6.self_attn.q_proj.weight\", \"model.encoder.layers.6.self_attn.q_proj.bias\", \"model.encoder.layers.6.self_attn.out_proj.weight\", \"model.encoder.layers.6.self_attn.out_proj.bias\", \"model.encoder.layers.6.self_attn_layer_norm.weight\", \"model.encoder.layers.6.self_attn_layer_norm.bias\", \"model.encoder.layers.6.fc1.weight\", \"model.encoder.layers.6.fc1.bias\", \"model.encoder.layers.6.fc2.weight\", \"model.encoder.layers.6.fc2.bias\", \"model.encoder.layers.6.final_layer_norm.weight\", \"model.encoder.layers.6.final_layer_norm.bias\", \"model.encoder.layers.7.self_attn.k_proj.weight\", \"model.encoder.layers.7.self_attn.k_proj.bias\", \"model.encoder.layers.7.self_attn.v_proj.weight\", \"model.encoder.layers.7.self_attn.v_proj.bias\", \"model.encoder.layers.7.self_attn.q_proj.weight\", \"model.encoder.layers.7.self_attn.q_proj.bias\", \"model.encoder.layers.7.self_attn.out_proj.weight\", \"model.encoder.layers.7.self_attn.out_proj.bias\", \"model.encoder.layers.7.self_attn_layer_norm.weight\", \"model.encoder.layers.7.self_attn_layer_norm.bias\", \"model.encoder.layers.7.fc1.weight\", \"model.encoder.layers.7.fc1.bias\", \"model.encoder.layers.7.fc2.weight\", \"model.encoder.layers.7.fc2.bias\", \"model.encoder.layers.7.final_layer_norm.weight\", \"model.encoder.layers.7.final_layer_norm.bias\", \"model.encoder.layers.8.self_attn.k_proj.weight\", \"model.encoder.layers.8.self_attn.k_proj.bias\", \"model.encoder.layers.8.self_attn.v_proj.weight\", \"model.encoder.layers.8.self_attn.v_proj.bias\", \"model.encoder.layers.8.self_attn.q_proj.weight\", \"model.encoder.layers.8.self_attn.q_proj.bias\", \"model.encoder.layers.8.self_attn.out_proj.weight\", \"model.encoder.layers.8.self_attn.out_proj.bias\", \"model.encoder.layers.8.self_attn_layer_norm.weight\", \"model.encoder.layers.8.self_attn_layer_norm.bias\", \"model.encoder.layers.8.fc1.weight\", \"model.encoder.layers.8.fc1.bias\", \"model.encoder.layers.8.fc2.weight\", \"model.encoder.layers.8.fc2.bias\", \"model.encoder.layers.8.final_layer_norm.weight\", \"model.encoder.layers.8.final_layer_norm.bias\", \"model.encoder.layers.9.self_attn.k_proj.weight\", \"model.encoder.layers.9.self_attn.k_proj.bias\", \"model.encoder.layers.9.self_attn.v_proj.weight\", \"model.encoder.layers.9.self_attn.v_proj.bias\", \"model.encoder.layers.9.self_attn.q_proj.weight\", \"model.encoder.layers.9.self_attn.q_proj.bias\", \"model.encoder.layers.9.self_attn.out_proj.weight\", \"model.encoder.layers.9.self_attn.out_proj.bias\", \"model.encoder.layers.9.self_attn_layer_norm.weight\", \"model.encoder.layers.9.self_attn_layer_norm.bias\", \"model.encoder.layers.9.fc1.weight\", \"model.encoder.layers.9.fc1.bias\", \"model.encoder.layers.9.fc2.weight\", \"model.encoder.layers.9.fc2.bias\", \"model.encoder.layers.9.final_layer_norm.weight\", \"model.encoder.layers.9.final_layer_norm.bias\", \"model.encoder.layers.10.self_attn.k_proj.weight\", \"model.encoder.layers.10.self_attn.k_proj.bias\", \"model.encoder.layers.10.self_attn.v_proj.weight\", \"model.encoder.layers.10.self_attn.v_proj.bias\", \"model.encoder.layers.10.self_attn.q_proj.weight\", \"model.encoder.layers.10.self_attn.q_proj.bias\", \"model.encoder.layers.10.self_attn.out_proj.weight\", \"model.encoder.layers.10.self_attn.out_proj.bias\", \"model.encoder.layers.10.self_attn_layer_norm.weight\", \"model.encoder.layers.10.self_attn_layer_norm.bias\", \"model.encoder.layers.10.fc1.weight\", \"model.encoder.layers.10.fc1.bias\", \"model.encoder.layers.10.fc2.weight\", \"model.encoder.layers.10.fc2.bias\", \"model.encoder.layers.10.final_layer_norm.weight\", \"model.encoder.layers.10.final_layer_norm.bias\", \"model.encoder.layers.11.self_attn.k_proj.weight\", \"model.encoder.layers.11.self_attn.k_proj.bias\", \"model.encoder.layers.11.self_attn.v_proj.weight\", \"model.encoder.layers.11.self_attn.v_proj.bias\", \"model.encoder.layers.11.self_attn.q_proj.weight\", \"model.encoder.layers.11.self_attn.q_proj.bias\", \"model.encoder.layers.11.self_attn.out_proj.weight\", \"model.encoder.layers.11.self_attn.out_proj.bias\", \"model.encoder.layers.11.self_attn_layer_norm.weight\", \"model.encoder.layers.11.self_attn_layer_norm.bias\", \"model.encoder.layers.11.fc1.weight\", \"model.encoder.layers.11.fc1.bias\", \"model.encoder.layers.11.fc2.weight\", \"model.encoder.layers.11.fc2.bias\", \"model.encoder.layers.11.final_layer_norm.weight\", \"model.encoder.layers.11.final_layer_norm.bias\", \"model.encoder.layernorm_embedding.weight\", \"model.encoder.layernorm_embedding.bias\", \"model.encoder.layer_norm.weight\", \"model.encoder.layer_norm.bias\", \"model.decoder.embed_tokens.weight\", \"model.decoder.embed_positions.weight\", \"model.decoder.layers.0.self_attn.k_proj.weight\", \"model.decoder.layers.0.self_attn.k_proj.bias\", \"model.decoder.layers.0.self_attn.v_proj.weight\", \"model.decoder.layers.0.self_attn.v_proj.bias\", \"model.decoder.layers.0.self_attn.q_proj.weight\", \"model.decoder.layers.0.self_attn.q_proj.bias\", \"model.decoder.layers.0.self_attn.out_proj.weight\", \"model.decoder.layers.0.self_attn.out_proj.bias\", \"model.decoder.layers.0.self_attn_layer_norm.weight\", \"model.decoder.layers.0.self_attn_layer_norm.bias\", \"model.decoder.layers.0.encoder_attn.k_proj.weight\", \"model.decoder.layers.0.encoder_attn.k_proj.bias\", \"model.decoder.layers.0.encoder_attn.v_proj.weight\", \"model.decoder.layers.0.encoder_attn.v_proj.bias\", \"model.decoder.layers.0.encoder_attn.q_proj.weight\", \"model.decoder.layers.0.encoder_attn.q_proj.bias\", \"model.decoder.layers.0.encoder_attn.out_proj.weight\", \"model.decoder.layers.0.encoder_attn.out_proj.bias\", \"model.decoder.layers.0.encoder_attn_layer_norm.weight\", \"model.decoder.layers.0.encoder_attn_layer_norm.bias\", \"model.decoder.layers.0.fc1.weight\", \"model.decoder.layers.0.fc1.bias\", \"model.decoder.layers.0.fc2.weight\", \"model.decoder.layers.0.fc2.bias\", \"model.decoder.layers.0.final_layer_norm.weight\", \"model.decoder.layers.0.final_layer_norm.bias\", \"model.decoder.layers.1.self_attn.k_proj.weight\", \"model.decoder.layers.1.self_attn.k_proj.bias\", \"model.decoder.layers.1.self_attn.v_proj.weight\", \"model.decoder.layers.1.self_attn.v_proj.bias\", \"model.decoder.layers.1.self_attn.q_proj.weight\", \"model.decoder.layers.1.self_attn.q_proj.bias\", \"model.decoder.layers.1.self_attn.out_proj.weight\", \"model.decoder.layers.1.self_attn.out_proj.bias\", \"model.decoder.layers.1.self_attn_layer_norm.weight\", \"model.decoder.layers.1.self_attn_layer_norm.bias\", \"model.decoder.layers.1.encoder_attn.k_proj.weight\", \"model.decoder.layers.1.encoder_attn.k_proj.bias\", \"model.decoder.layers.1.encoder_attn.v_proj.weight\", \"model.decoder.layers.1.encoder_attn.v_proj.bias\", \"model.decoder.layers.1.encoder_attn.q_proj.weight\", \"model.decoder.layers.1.encoder_attn.q_proj.bias\", \"model.decoder.layers.1.encoder_attn.out_proj.weight\", \"model.decoder.layers.1.encoder_attn.out_proj.bias\", \"model.decoder.layers.1.encoder_attn_layer_norm.weight\", \"model.decoder.layers.1.encoder_attn_layer_norm.bias\", \"model.decoder.layers.1.fc1.weight\", \"model.decoder.layers.1.fc1.bias\", \"model.decoder.layers.1.fc2.weight\", \"model.decoder.layers.1.fc2.bias\", \"model.decoder.layers.1.final_layer_norm.weight\", \"model.decoder.layers.1.final_layer_norm.bias\", \"model.decoder.layers.2.self_attn.k_proj.weight\", \"model.decoder.layers.2.self_attn.k_proj.bias\", \"model.decoder.layers.2.self_attn.v_proj.weight\", \"model.decoder.layers.2.self_attn.v_proj.bias\", \"model.decoder.layers.2.self_attn.q_proj.weight\", \"model.decoder.layers.2.self_attn.q_proj.bias\", \"model.decoder.layers.2.self_attn.out_proj.weight\", \"model.decoder.layers.2.self_attn.out_proj.bias\", \"model.decoder.layers.2.self_attn_layer_norm.weight\", \"model.decoder.layers.2.self_attn_layer_norm.bias\", \"model.decoder.layers.2.encoder_attn.k_proj.weight\", \"model.decoder.layers.2.encoder_attn.k_proj.bias\", \"model.decoder.layers.2.encoder_attn.v_proj.weight\", \"model.decoder.layers.2.encoder_attn.v_proj.bias\", \"model.decoder.layers.2.encoder_attn.q_proj.weight\", \"model.decoder.layers.2.encoder_attn.q_proj.bias\", \"model.decoder.layers.2.encoder_attn.out_proj.weight\", \"model.decoder.layers.2.encoder_attn.out_proj.bias\", \"model.decoder.layers.2.encoder_attn_layer_norm.weight\", \"model.decoder.layers.2.encoder_attn_layer_norm.bias\", \"model.decoder.layers.2.fc1.weight\", \"model.decoder.layers.2.fc1.bias\", \"model.decoder.layers.2.fc2.weight\", \"model.decoder.layers.2.fc2.bias\", \"model.decoder.layers.2.final_layer_norm.weight\", \"model.decoder.layers.2.final_layer_norm.bias\", \"model.decoder.layers.3.self_attn.k_proj.weight\", \"model.decoder.layers.3.self_attn.k_proj.bias\", \"model.decoder.layers.3.self_attn.v_proj.weight\", \"model.decoder.layers.3.self_attn.v_proj.bias\", \"model.decoder.layers.3.self_attn.q_proj.weight\", \"model.decoder.layers.3.self_attn.q_proj.bias\", \"model.decoder.layers.3.self_attn.out_proj.weight\", \"model.decoder.layers.3.self_attn.out_proj.bias\", \"model.decoder.layers.3.self_attn_layer_norm.weight\", \"model.decoder.layers.3.self_attn_layer_norm.bias\", \"model.decoder.layers.3.encoder_attn.k_proj.weight\", \"model.decoder.layers.3.encoder_attn.k_proj.bias\", \"model.decoder.layers.3.encoder_attn.v_proj.weight\", \"model.decoder.layers.3.encoder_attn.v_proj.bias\", \"model.decoder.layers.3.encoder_attn.q_proj.weight\", \"model.decoder.layers.3.encoder_attn.q_proj.bias\", \"model.decoder.layers.3.encoder_attn.out_proj.weight\", \"model.decoder.layers.3.encoder_attn.out_proj.bias\", \"model.decoder.layers.3.encoder_attn_layer_norm.weight\", \"model.decoder.layers.3.encoder_attn_layer_norm.bias\", \"model.decoder.layers.3.fc1.weight\", \"model.decoder.layers.3.fc1.bias\", \"model.decoder.layers.3.fc2.weight\", \"model.decoder.layers.3.fc2.bias\", \"model.decoder.layers.3.final_layer_norm.weight\", \"model.decoder.layers.3.final_layer_norm.bias\", \"model.decoder.layers.4.self_attn.k_proj.weight\", \"model.decoder.layers.4.self_attn.k_proj.bias\", \"model.decoder.layers.4.self_attn.v_proj.weight\", \"model.decoder.layers.4.self_attn.v_proj.bias\", \"model.decoder.layers.4.self_attn.q_proj.weight\", \"model.decoder.layers.4.self_attn.q_proj.bias\", \"model.decoder.layers.4.self_attn.out_proj.weight\", \"model.decoder.layers.4.self_attn.out_proj.bias\", \"model.decoder.layers.4.self_attn_layer_norm.weight\", \"model.decoder.layers.4.self_attn_layer_norm.bias\", \"model.decoder.layers.4.encoder_attn.k_proj.weight\", \"model.decoder.layers.4.encoder_attn.k_proj.bias\", \"model.decoder.layers.4.encoder_attn.v_proj.weight\", \"model.decoder.layers.4.encoder_attn.v_proj.bias\", \"model.decoder.layers.4.encoder_attn.q_proj.weight\", \"model.decoder.layers.4.encoder_attn.q_proj.bias\", \"model.decoder.layers.4.encoder_attn.out_proj.weight\", \"model.decoder.layers.4.encoder_attn.out_proj.bias\", \"model.decoder.layers.4.encoder_attn_layer_norm.weight\", \"model.decoder.layers.4.encoder_attn_layer_norm.bias\", \"model.decoder.layers.4.fc1.weight\", \"model.decoder.layers.4.fc1.bias\", \"model.decoder.layers.4.fc2.weight\", \"model.decoder.layers.4.fc2.bias\", \"model.decoder.layers.4.final_layer_norm.weight\", \"model.decoder.layers.4.final_layer_norm.bias\", \"model.decoder.layers.5.self_attn.k_proj.weight\", \"model.decoder.layers.5.self_attn.k_proj.bias\", \"model.decoder.layers.5.self_attn.v_proj.weight\", \"model.decoder.layers.5.self_attn.v_proj.bias\", \"model.decoder.layers.5.self_attn.q_proj.weight\", \"model.decoder.layers.5.self_attn.q_proj.bias\", \"model.decoder.layers.5.self_attn.out_proj.weight\", \"model.decoder.layers.5.self_attn.out_proj.bias\", \"model.decoder.layers.5.self_attn_layer_norm.weight\", \"model.decoder.layers.5.self_attn_layer_norm.bias\", \"model.decoder.layers.5.encoder_attn.k_proj.weight\", \"model.decoder.layers.5.encoder_attn.k_proj.bias\", \"model.decoder.layers.5.encoder_attn.v_proj.weight\", \"model.decoder.layers.5.encoder_attn.v_proj.bias\", \"model.decoder.layers.5.encoder_attn.q_proj.weight\", \"model.decoder.layers.5.encoder_attn.q_proj.bias\", \"model.decoder.layers.5.encoder_attn.out_proj.weight\", \"model.decoder.layers.5.encoder_attn.out_proj.bias\", \"model.decoder.layers.5.encoder_attn_layer_norm.weight\", \"model.decoder.layers.5.encoder_attn_layer_norm.bias\", \"model.decoder.layers.5.fc1.weight\", \"model.decoder.layers.5.fc1.bias\", \"model.decoder.layers.5.fc2.weight\", \"model.decoder.layers.5.fc2.bias\", \"model.decoder.layers.5.final_layer_norm.weight\", \"model.decoder.layers.5.final_layer_norm.bias\", \"model.decoder.layers.6.self_attn.k_proj.weight\", \"model.decoder.layers.6.self_attn.k_proj.bias\", \"model.decoder.layers.6.self_attn.v_proj.weight\", \"model.decoder.layers.6.self_attn.v_proj.bias\", \"model.decoder.layers.6.self_attn.q_proj.weight\", \"model.decoder.layers.6.self_attn.q_proj.bias\", \"model.decoder.layers.6.self_attn.out_proj.weight\", \"model.decoder.layers.6.self_attn.out_proj.bias\", \"model.decoder.layers.6.self_attn_layer_norm.weight\", \"model.decoder.layers.6.self_attn_layer_norm.bias\", \"model.decoder.layers.6.encoder_attn.k_proj.weight\", \"model.decoder.layers.6.encoder_attn.k_proj.bias\", \"model.decoder.layers.6.encoder_attn.v_proj.weight\", \"model.decoder.layers.6.encoder_attn.v_proj.bias\", \"model.decoder.layers.6.encoder_attn.q_proj.weight\", \"model.decoder.layers.6.encoder_attn.q_proj.bias\", \"model.decoder.layers.6.encoder_attn.out_proj.weight\", \"model.decoder.layers.6.encoder_attn.out_proj.bias\", \"model.decoder.layers.6.encoder_attn_layer_norm.weight\", \"model.decoder.layers.6.encoder_attn_layer_norm.bias\", \"model.decoder.layers.6.fc1.weight\", \"model.decoder.layers.6.fc1.bias\", \"model.decoder.layers.6.fc2.weight\", \"model.decoder.layers.6.fc2.bias\", \"model.decoder.layers.6.final_layer_norm.weight\", \"model.decoder.layers.6.final_layer_norm.bias\", \"model.decoder.layers.7.self_attn.k_proj.weight\", \"model.decoder.layers.7.self_attn.k_proj.bias\", \"model.decoder.layers.7.self_attn.v_proj.weight\", \"model.decoder.layers.7.self_attn.v_proj.bias\", \"model.decoder.layers.7.self_attn.q_proj.weight\", \"model.decoder.layers.7.self_attn.q_proj.bias\", \"model.decoder.layers.7.self_attn.out_proj.weight\", \"model.decoder.layers.7.self_attn.out_proj.bias\", \"model.decoder.layers.7.self_attn_layer_norm.weight\", \"model.decoder.layers.7.self_attn_layer_norm.bias\", \"model.decoder.layers.7.encoder_attn.k_proj.weight\", \"model.decoder.layers.7.encoder_attn.k_proj.bias\", \"model.decoder.layers.7.encoder_attn.v_proj.weight\", \"model.decoder.layers.7.encoder_attn.v_proj.bias\", \"model.decoder.layers.7.encoder_attn.q_proj.weight\", \"model.decoder.layers.7.encoder_attn.q_proj.bias\", \"model.decoder.layers.7.encoder_attn.out_proj.weight\", \"model.decoder.layers.7.encoder_attn.out_proj.bias\", \"model.decoder.layers.7.encoder_attn_layer_norm.weight\", \"model.decoder.layers.7.encoder_attn_layer_norm.bias\", \"model.decoder.layers.7.fc1.weight\", \"model.decoder.layers.7.fc1.bias\", \"model.decoder.layers.7.fc2.weight\", \"model.decoder.layers.7.fc2.bias\", \"model.decoder.layers.7.final_layer_norm.weight\", \"model.decoder.layers.7.final_layer_norm.bias\", \"model.decoder.layers.8.self_attn.k_proj.weight\", \"model.decoder.layers.8.self_attn.k_proj.bias\", \"model.decoder.layers.8.self_attn.v_proj.weight\", \"model.decoder.layers.8.self_attn.v_proj.bias\", \"model.decoder.layers.8.self_attn.q_proj.weight\", \"model.decoder.layers.8.self_attn.q_proj.bias\", \"model.decoder.layers.8.self_attn.out_proj.weight\", \"model.decoder.layers.8.self_attn.out_proj.bias\", \"model.decoder.layers.8.self_attn_layer_norm.weight\", \"model.decoder.layers.8.self_attn_layer_norm.bias\", \"model.decoder.layers.8.encoder_attn.k_proj.weight\", \"model.decoder.layers.8.encoder_attn.k_proj.bias\", \"model.decoder.layers.8.encoder_attn.v_proj.weight\", \"model.decoder.layers.8.encoder_attn.v_proj.bias\", \"model.decoder.layers.8.encoder_attn.q_proj.weight\", \"model.decoder.layers.8.encoder_attn.q_proj.bias\", \"model.decoder.layers.8.encoder_attn.out_proj.weight\", \"model.decoder.layers.8.encoder_attn.out_proj.bias\", \"model.decoder.layers.8.encoder_attn_layer_norm.weight\", \"model.decoder.layers.8.encoder_attn_layer_norm.bias\", \"model.decoder.layers.8.fc1.weight\", \"model.decoder.layers.8.fc1.bias\", \"model.decoder.layers.8.fc2.weight\", \"model.decoder.layers.8.fc2.bias\", \"model.decoder.layers.8.final_layer_norm.weight\", \"model.decoder.layers.8.final_layer_norm.bias\", \"model.decoder.layers.9.self_attn.k_proj.weight\", \"model.decoder.layers.9.self_attn.k_proj.bias\", \"model.decoder.layers.9.self_attn.v_proj.weight\", \"model.decoder.layers.9.self_attn.v_proj.bias\", \"model.decoder.layers.9.self_attn.q_proj.weight\", \"model.decoder.layers.9.self_attn.q_proj.bias\", \"model.decoder.layers.9.self_attn.out_proj.weight\", \"model.decoder.layers.9.self_attn.out_proj.bias\", \"model.decoder.layers.9.self_attn_layer_norm.weight\", \"model.decoder.layers.9.self_attn_layer_norm.bias\", \"model.decoder.layers.9.encoder_attn.k_proj.weight\", \"model.decoder.layers.9.encoder_attn.k_proj.bias\", \"model.decoder.layers.9.encoder_attn.v_proj.weight\", \"model.decoder.layers.9.encoder_attn.v_proj.bias\", \"model.decoder.layers.9.encoder_attn.q_proj.weight\", \"model.decoder.layers.9.encoder_attn.q_proj.bias\", \"model.decoder.layers.9.encoder_attn.out_proj.weight\", \"model.decoder.layers.9.encoder_attn.out_proj.bias\", \"model.decoder.layers.9.encoder_attn_layer_norm.weight\", \"model.decoder.layers.9.encoder_attn_layer_norm.bias\", \"model.decoder.layers.9.fc1.weight\", \"model.decoder.layers.9.fc1.bias\", \"model.decoder.layers.9.fc2.weight\", \"model.decoder.layers.9.fc2.bias\", \"model.decoder.layers.9.final_layer_norm.weight\", \"model.decoder.layers.9.final_layer_norm.bias\", \"model.decoder.layers.10.self_attn.k_proj.weight\", \"model.decoder.layers.10.self_attn.k_proj.bias\", \"model.decoder.layers.10.self_attn.v_proj.weight\", \"model.decoder.layers.10.self_attn.v_proj.bias\", \"model.decoder.layers.10.self_attn.q_proj.weight\", \"model.decoder.layers.10.self_attn.q_proj.bias\", \"model.decoder.layers.10.self_attn.out_proj.weight\", \"model.decoder.layers.10.self_attn.out_proj.bias\", \"model.decoder.layers.10.self_attn_layer_norm.weight\", \"model.decoder.layers.10.self_attn_layer_norm.bias\", \"model.decoder.layers.10.encoder_attn.k_proj.weight\", \"model.decoder.layers.10.encoder_attn.k_proj.bias\", \"model.decoder.layers.10.encoder_attn.v_proj.weight\", \"model.decoder.layers.10.encoder_attn.v_proj.bias\", \"model.decoder.layers.10.encoder_attn.q_proj.weight\", \"model.decoder.layers.10.encoder_attn.q_proj.bias\", \"model.decoder.layers.10.encoder_attn.out_proj.weight\", \"model.decoder.layers.10.encoder_attn.out_proj.bias\", \"model.decoder.layers.10.encoder_attn_layer_norm.weight\", \"model.decoder.layers.10.encoder_attn_layer_norm.bias\", \"model.decoder.layers.10.fc1.weight\", \"model.decoder.layers.10.fc1.bias\", \"model.decoder.layers.10.fc2.weight\", \"model.decoder.layers.10.fc2.bias\", \"model.decoder.layers.10.final_layer_norm.weight\", \"model.decoder.layers.10.final_layer_norm.bias\", \"model.decoder.layers.11.self_attn.k_proj.weight\", \"model.decoder.layers.11.self_attn.k_proj.bias\", \"model.decoder.layers.11.self_attn.v_proj.weight\", \"model.decoder.layers.11.self_attn.v_proj.bias\", \"model.decoder.layers.11.self_attn.q_proj.weight\", \"model.decoder.layers.11.self_attn.q_proj.bias\", \"model.decoder.layers.11.self_attn.out_proj.weight\", \"model.decoder.layers.11.self_attn.out_proj.bias\", \"model.decoder.layers.11.self_attn_layer_norm.weight\", \"model.decoder.layers.11.self_attn_layer_norm.bias\", \"model.decoder.layers.11.encoder_attn.k_proj.weight\", \"model.decoder.layers.11.encoder_attn.k_proj.bias\", \"model.decoder.layers.11.encoder_attn.v_proj.weight\", \"model.decoder.layers.11.encoder_attn.v_proj.bias\", \"model.decoder.layers.11.encoder_attn.q_proj.weight\", \"model.decoder.layers.11.encoder_attn.q_proj.bias\", \"model.decoder.layers.11.encoder_attn.out_proj.weight\", \"model.decoder.layers.11.encoder_attn.out_proj.bias\", \"model.decoder.layers.11.encoder_attn_layer_norm.weight\", \"model.decoder.layers.11.encoder_attn_layer_norm.bias\", \"model.decoder.layers.11.fc1.weight\", \"model.decoder.layers.11.fc1.bias\", \"model.decoder.layers.11.fc2.weight\", \"model.decoder.layers.11.fc2.bias\", \"model.decoder.layers.11.final_layer_norm.weight\", \"model.decoder.layers.11.final_layer_norm.bias\", \"model.decoder.layernorm_embedding.weight\", \"model.decoder.layernorm_embedding.bias\", \"model.decoder.layer_norm.weight\", \"model.decoder.layer_norm.bias\", \"lm_head.weight\". \n\tUnexpected key(s) in state_dict: \"args\", \"model\", \"optimizer_history\", \"extra_state\", \"last_optimizer_state\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "import sentencepiece as spm\n",
    "# Path to your model checkpoint\n",
    "model_path = \"checkpoints/zmbart\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=model_path+\"/spiece.model\")\n",
    "\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\").to(device)\n",
    "\n",
    "# Load the fine-tuned weights\n",
    "model.load_state_dict(torch.load(model_path + \"/zmbart_checkpoint112.pt\"))\n",
    "\n",
    "def safe_decode(tokenizer, output_ids):\n",
    "    decoded_tokens = []\n",
    "    \n",
    "    for token_id in output_ids:\n",
    "        try:\n",
    "            # Decode each token individually\n",
    "            decoded_token = tokenizer.decode(token_id.item(), skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "            \n",
    "            # If token is padding or unknown, append an empty string or placeholder\n",
    "            if decoded_token == \"<pad>\" or decoded_token == \"<unk>\":\n",
    "                decoded_tokens.append(\"\")\n",
    "            else:\n",
    "                decoded_tokens.append(decoded_token)\n",
    "        except IndexError:\n",
    "            # Handle index errors by appending an empty string\n",
    "            decoded_tokens.append(\"\")\n",
    "    \n",
    "    # Join tokens with spaces, replacing empty tokens appropriately\n",
    "    return ' '.join(decoded_tokens).strip().replace('▁', ' ').replace('  ', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta XNLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Path to your model checkpoint\n",
    "checkpoint_path = \"/workspace/Tesis/O3_modelos/checkpoints/metaXNLG_checkpoint-10500/\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
    "\n",
    "def safe_decode(tokenizer, output_ids):\n",
    "    # print(output_ids)\n",
    "    decoded_tokens = []\n",
    "    \n",
    "    for token_id in output_ids:\n",
    "        try:\n",
    "            # Decode each token individually\n",
    "            decoded_token = tokenizer.decode(token_id.item(), skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "            \n",
    "            # If token is padding or unknown, append an empty string or placeholder\n",
    "            if decoded_token == \"<pad>\" or decoded_token == \"<unk>\":\n",
    "                decoded_tokens.append(\"\")\n",
    "            else:\n",
    "                decoded_tokens.append(decoded_token)\n",
    "        except IndexError:\n",
    "            # Handle index errors by appending an empty string\n",
    "            decoded_tokens.append(\"\")\n",
    "    \n",
    "    # Join tokens with spaces, replacing empty tokens appropriately\n",
    "    return ' '.join(decoded_tokens).strip().replace('▁', ' ').replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a number of input texts\n",
    "num_samples = 1\n",
    "import random\n",
    "\n",
    "\n",
    "sentences = []\n",
    "\n",
    "while len(sentences) <= 100:\n",
    "    input_texts = random.sample(unique_words_list, num_samples) \n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Move inputs to GPU\n",
    "    outputs = model.generate(**inputs, max_length=30)\n",
    "    predictions = [safe_decode(tokenizer, output) for output in outputs]\n",
    "    split_sentences = [sentence.split('</s>') for sentence in predictions]\n",
    "    split_sentences = [[s.strip() for s in sentence_list if s.strip()] for sentence_list in split_sentences]\n",
    "    flattened_sentences = [item for sublist in split_sentences for item in sublist]\n",
    "    sentences.extend(flattened_sentences)\n",
    "    print(\"Tenemos \",len(sentences),\" oraciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionarioGenerado[\"metaXNLG\"]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas\n",
    "perplexity = calculate_perplexity(model, tokenizer, sentences, device)\n",
    "distinct_2 = calculate_distinct_n(sentences, n=2)\n",
    "distinct_3 = calculate_distinct_n(sentences, n=3)\n",
    "mauve_score = calculate_mauve(sentences)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Distinct-2: {distinct_2}\")\n",
    "print(f\"Distinct-3: {distinct_3}\")\n",
    "print(f\"MAUVE: {mauve_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, pretrained_embeddings):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(pretrained_embeddings)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Use last output from LSTM\n",
    "        return output\n",
    "\n",
    "def test_lstm(model, input_data, vocab):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor([[vocab[token] for token in sentence] for sentence in input_data])\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "    return predictions\n",
    "\n",
    "# Load model with trained weights\n",
    "vocab_size = len(vocab)\n",
    "embed_size = pretrained_embeddings.shape[1]\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size=128, output_size=vocab_size, pretrained_embeddings=pretrained_embeddings)\n",
    "\n",
    "input_data = [['your', 'input', 'sentence']]\n",
    "lstm_predictions = test_lstm(model, input_data, vocab)\n",
    "print(lstm_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
