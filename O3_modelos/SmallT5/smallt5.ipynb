{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "# config = T5Config(\n",
    "#     d_model=512,  # Embedding dimension\n",
    "#     d_ff=2048,    # Feed-forward layer size\n",
    "#     num_layers=8,  # Number of encoder and decoder layers\n",
    "#     num_heads=8,   # Attention heads\n",
    "#     vocab_size=32128  # Size of the vocabulary\n",
    "# )\n",
    "\n",
    "# model = T5ForConditionalGeneration(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Load the vocabulary (token -> index) mapping\n",
    "vocab = {}  # You should populate this with your BPE vocabulary\n",
    "with open('tokenizadorIskonawa.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for idx, line in enumerate(vocab_file):\n",
    "        token, code = re.split(r'\\t', line.strip())\n",
    "        # Save as integer\n",
    "        vocab[token] = idx\n",
    "\n",
    "# Load the BPE tokenized dataset\n",
    "def load_bpe_dataset(file_path, vocab):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Tokenize each line into subwords and convert them to indices\n",
    "            tokens = line.strip().split()  # Assuming tokens are space-separated\n",
    "            token_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]  # Handle unknown tokens\n",
    "            dataset.append(token_ids)\n",
    "    return dataset\n",
    "\n",
    "bpe_tokenized_dataset = load_bpe_dataset('tokens.txt', vocab)\n",
    "\n",
    "# Check dataset example\n",
    "print(bpe_tokenized_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.2\n",
    "\n",
    "for split in ['eval', 'train']:\n",
    "    corpus = 'tokens.txt'\n",
    "    output = f'iskCorpus.t5.{split}.tsv'\n",
    "    with open(corpus, 'r') as reader:\n",
    "        lines = reader.readlines()\n",
    "        total_lines = len(lines)\n",
    "        split_index = int(total_lines * split_ratio)\n",
    "        \n",
    "        with open(output, 'w') as writer:\n",
    "            writer.write('input_text\\ttarget_text\\n')\n",
    "            for number, line in enumerate(lines):\n",
    "                if split == 'train' and number >= split_index:\n",
    "                    line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                    writer.write(line+'\\t'+line+'\\n')\n",
    "                elif split == 'eval' and number < split_index:\n",
    "                    line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                    writer.write(line+'\\t'+line+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "# import seqio\n",
    "# import t5.data\n",
    "# from t5.data import preprocessors\n",
    "# import tensorflow as tf\n",
    "# TaskRegistry = seqio.TaskRegistry\n",
    "# MixtureRegistry = seqio.MixtureRegistry\n",
    "\n",
    "# # Define your dataset splits\n",
    "# datasplit = {\n",
    "#     \"train\": \"iskCorpus.t5.train.tsv\",\n",
    "#     \"validation\": \"iskCorpus.t5.eval.tsv\"\n",
    "# }\n",
    "\n",
    "# DEFAULT_OUTPUT_FEATURES = {\n",
    "#     \"inputs\": seqio.Feature(\n",
    "#         seqio.SentencePieceVocabulary(vocab), add_eos=True,\n",
    "#         required=False, dtype=tf.int32),\n",
    "#     \"targets\": seqio.Feature(\n",
    "#         seqio.SentencePieceVocabulary(vocab), add_eos=True, dtype=tf.int32)\n",
    "# }\n",
    "\n",
    "# # Add the text generation tasks\n",
    "# TaskRegistry.add(\n",
    "#     \"text_generation_span_corruption\",\n",
    "#     source=seqio.TextLineDataSource(split_to_filepattern=datasplit),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(preprocessors.parse_tsv),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         preprocessors.span_corruption,  # 15% span corruption\n",
    "#         seqio.preprocessors.append_eos_after_trim,\n",
    "#     ],\n",
    "#     output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "#     metric_fns=[]  # Add evaluation metrics if needed\n",
    "# )\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"text_generation_iid_denoising\",\n",
    "#     source=seqio.TextLineDataSource(split_to_filepattern=datasplit),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(preprocessors.parse_tsv),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         preprocessors.iid_denoising,    # 15% i.i.d. denoising\n",
    "#         seqio.preprocessors.append_eos_after_trim,\n",
    "#     ],\n",
    "#     output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "#     metric_fns=[]  # Add evaluation metrics if needed\n",
    "# )\n",
    "\n",
    "# # Mixture of both tasks\n",
    "# MixtureRegistry.add(\n",
    "#     \"text_generation_mixture\",\n",
    "#     [\"text_generation_span_corruption\", \"text_generation_iid_denoising\"],\n",
    "#     default_rate=1.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/harvymartinez16_gmail_com/.local/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2023080216raw_log_internal21internal_log_functionB5cxx11E",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgin\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/t5/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The T5 Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Import API modules.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Version number.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/t5/data/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Import data modules.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# pylint:disable=wildcard-import,g-bad-import-order\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_providers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglue_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpostprocessors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/t5/data/dataset_providers.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mapping\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/__init__.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Import to top-level API.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# pylint:disable=wildcard-import,g-bad-import-order\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_providers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluation\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/dataset_providers.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m version_lib\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglove\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpg\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m metrics_lib\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessors \u001b[38;5;28;01mas\u001b[39;00m seqio_preprocessors\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m task_registry_provenance_tracking\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/metrics.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@dataclasses\u001b[39m\u001b[38;5;241m.\u001b[39mdataclass\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMetricValue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/utils.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocabularies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocabulary\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/seqio/vocabularies.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf_text\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentencepiece_model_pb2\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msentencepiece_processor\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_text/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_undocumented\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpybinds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tflite_registrar\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n",
      "\u001b[0;31mImportError\u001b[0m: /home/harvymartinez16_gmail_com/.local/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2023080216raw_log_internal21internal_log_functionB5cxx11E"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from t5 import models\n",
    "import gin\n",
    "import logging\n",
    "\n",
    "# Set up your environment variables and paths\n",
    "MODEL_DIR = \"../t5_sl_small\"  # Change to your desired model directory\n",
    "os.environ[\"PYTHONPATH\"] = f\"{os.environ['PYTHONPATH']}:/text-to-text-transfer-transformer:/text-to-text-transfer-transformer/t5\"\n",
    "\n",
    "# Define training parameters\n",
    "train_steps = 1000000\n",
    "save_checkpoints_steps = 40000\n",
    "keep_checkpoint_max = 2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up the gin configuration for the model and dataset\n",
    "# You can create your .gin files or configure them directly in the notebook.\n",
    "gin.parse_config_file(\"/config/dataset.gin\")\n",
    "gin.parse_config_file(\"/config/t5.1.1.small.gin\")\n",
    "gin.bind_parameter(\"utils.run.mesh_shape\", \"model:2,batch:2\")\n",
    "gin.bind_parameter(\"utils.run.mesh_devices\", [\"tpu:0\"])\n",
    "gin.bind_parameter(\"MIXTURE_NAME\", \"mixture_iskonawa_test\")  # Your mixture name\n",
    "gin.bind_parameter(\"utils.run.save_checkpoints_steps\", save_checkpoints_steps)\n",
    "gin.bind_parameter(\"utils.run.keep_checkpoint_max\", keep_checkpoint_max)\n",
    "gin.bind_parameter(\"utils.run.train_steps\", train_steps)\n",
    "\n",
    "class ProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logger.info(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}\")\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 100 == 0:  # Log every 1000 steps\n",
    "            logger.info(f\"Step {batch}: loss = {logs.get('loss', 'N/A'):.4f}, accuracy = {logs.get('accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "# Start the training\n",
    "with strategy.scope():\n",
    "    models.t5_mesh_transformer(\n",
    "        model_dir=MODEL_DIR,\n",
    "        gin_file=\"/config/dataset.gin\",\n",
    "        gin_file=\"/config/t5.1.1.small.gin\",\n",
    "        module_import=\"/config/mytask3\",  \n",
    "        callbacks=[ProgressCallback()]\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
