{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "# config = T5Config(\n",
    "#     d_model=512,  # Embedding dimension\n",
    "#     d_ff=2048,    # Feed-forward layer size\n",
    "#     num_layers=8,  # Number of encoder and decoder layers\n",
    "#     num_heads=8,   # Attention heads\n",
    "#     vocab_size=32128  # Size of the vocabulary\n",
    "# )\n",
    "\n",
    "# model = T5ForConditionalGeneration(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Load the vocabulary (token -> index) mapping\n",
    "vocab = {}  # You should populate this with your BPE vocabulary\n",
    "with open('tokenizadorIskonawa.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for idx, line in enumerate(vocab_file):\n",
    "        token, code = re.split(r'\\t', line.strip())\n",
    "        # Save as integer\n",
    "        vocab[token] = idx\n",
    "\n",
    "# Load the BPE tokenized dataset\n",
    "def load_bpe_dataset(file_path, vocab):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Tokenize each line into subwords and convert them to indices\n",
    "            tokens = line.strip().split()  # Assuming tokens are space-separated\n",
    "            token_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]  # Handle unknown tokens\n",
    "            dataset.append(token_ids)\n",
    "    return dataset\n",
    "\n",
    "bpe_tokenized_dataset = load_bpe_dataset('tokens.txt', vocab)\n",
    "\n",
    "# Check dataset example\n",
    "print(bpe_tokenized_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.2\n",
    "\n",
    "for split in ['eval', 'train']:\n",
    "    corpus = 'tokens.txt'\n",
    "    output = f'iskCorpus.t5.{split}.tsv'\n",
    "    with open(corpus, 'r') as reader:\n",
    "        lines = reader.readlines()\n",
    "        total_lines = len(lines)\n",
    "        split_index = int(total_lines * split_ratio)\n",
    "        \n",
    "        with open(output, 'w') as writer:\n",
    "            writer.write('input_text\\ttarget_text\\n')\n",
    "            for number, line in enumerate(lines):\n",
    "                if split == 'train' and number >= split_index:\n",
    "                    line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                    writer.write(line+'\\t'+line+'\\n')\n",
    "                elif split == 'eval' and number < split_index:\n",
    "                    line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                    writer.write(line+'\\t'+line+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "# import seqio\n",
    "# import t5.data\n",
    "# from t5.data import preprocessors\n",
    "# import tensorflow as tf\n",
    "# TaskRegistry = seqio.TaskRegistry\n",
    "# MixtureRegistry = seqio.MixtureRegistry\n",
    "\n",
    "# # Define your dataset splits\n",
    "# datasplit = {\n",
    "#     \"train\": \"iskCorpus.t5.train.tsv\",\n",
    "#     \"validation\": \"iskCorpus.t5.eval.tsv\"\n",
    "# }\n",
    "\n",
    "# DEFAULT_OUTPUT_FEATURES = {\n",
    "#     \"inputs\": seqio.Feature(\n",
    "#         seqio.SentencePieceVocabulary(vocab), add_eos=True,\n",
    "#         required=False, dtype=tf.int32),\n",
    "#     \"targets\": seqio.Feature(\n",
    "#         seqio.SentencePieceVocabulary(vocab), add_eos=True, dtype=tf.int32)\n",
    "# }\n",
    "\n",
    "# # Add the text generation tasks\n",
    "# TaskRegistry.add(\n",
    "#     \"text_generation_span_corruption\",\n",
    "#     source=seqio.TextLineDataSource(split_to_filepattern=datasplit),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(preprocessors.parse_tsv),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         preprocessors.span_corruption,  # 15% span corruption\n",
    "#         seqio.preprocessors.append_eos_after_trim,\n",
    "#     ],\n",
    "#     output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "#     metric_fns=[]  # Add evaluation metrics if needed\n",
    "# )\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"text_generation_iid_denoising\",\n",
    "#     source=seqio.TextLineDataSource(split_to_filepattern=datasplit),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(preprocessors.parse_tsv),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         preprocessors.iid_denoising,    # 15% i.i.d. denoising\n",
    "#         seqio.preprocessors.append_eos_after_trim,\n",
    "#     ],\n",
    "#     output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "#     metric_fns=[]  # Add evaluation metrics if needed\n",
    "# )\n",
    "\n",
    "# # Mixture of both tasks\n",
    "# MixtureRegistry.add(\n",
    "#     \"text_generation_mixture\",\n",
    "#     [\"text_generation_span_corruption\", \"text_generation_iid_denoising\"],\n",
    "#     default_rate=1.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/harvymartinez16_gmail_com/Tesis/\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "# List and print GPU devices available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPU found.\")\n",
    "\n",
    "# Define a function for addition using @tf.function\n",
    "@tf.function\n",
    "def add_fn(x, y):\n",
    "    z = x + y\n",
    "    return z\n",
    "\n",
    "# Example of running the function\n",
    "x = tf.constant(2.0)\n",
    "y = tf.constant(3.0)\n",
    "result = add_fn(x, y)\n",
    "print(\"Result: \", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import t5\n",
    "import gin\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Set up your environment variables and paths\n",
    "MODEL_DIR = \"t5_sl_small\"  # Change to your desired model directory\n",
    "# os.environ[\"PYTHONPATH\"] = f\"{os.environ['PYTHONPATH']}:/text-to-text-transfer-transformer:/text-to-text-transfer-transformer/t5\"\n",
    "\n",
    "# Define training parameters\n",
    "train_steps = 1000000\n",
    "save_checkpoints_steps = 40000\n",
    "keep_checkpoint_max = 2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up the gin configuration for the model and dataset\n",
    "# You can create your .gin files or configure them directly in the notebook.\n",
    "gin.parse_config_file(\"config/dataset.gin\")\n",
    "gin.parse_config_file(\"config/t5.1.1.small.gin\")\n",
    "gin.bind_parameter(\"utils.run.mesh_shape\", \"model:1,batch:1\")\n",
    "gin.bind_parameter(\"utils.run.mesh_devices\", [\"GPU:0\"])\n",
    "# gin.bind_parameter(\"MIXTURE_NAME\", \"mixture_iskonawa_test\")  # Your mixture name\n",
    "gin.bind_parameter(\"utils.run.save_checkpoints_steps\", save_checkpoints_steps)\n",
    "gin.bind_parameter(\"utils.run.keep_checkpoint_max\", keep_checkpoint_max)\n",
    "gin.bind_parameter(\"utils.run.train_steps\", train_steps)\n",
    "\n",
    "class ProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logger.info(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}\")\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 100 == 0:  # Log every 1000 steps\n",
    "            logger.info(f\"Step {batch}: loss = {logs.get('loss', 'N/A'):.4f}, accuracy = {logs.get('accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy()  # Supports multi-GPU, defaults to single GPU if only one available\n",
    "\n",
    "# # Start the training\n",
    "# with strategy.scope():\n",
    "#     mesh_transformer(\n",
    "#         model_dir=MODEL_DIR,\n",
    "#         gin_file=[\"/config/dataset.gin\",\"/config/t5.1.1.small.gin\"],\n",
    "#         module_import=\"/config/mytask3\",  \n",
    "#         callbacks=[ProgressCallback()]\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Tesis/O3_modelos/SmallT5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Execute the command\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrors:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:2021\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2015\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2016\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2021\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# Define the model directory\n",
    "MODEL_DIR = \"t5_isk_small\"\n",
    "train_steps = 1000000\n",
    "save_checkpoints_steps = 40000\n",
    "keep_checkpoint_max = 2 \n",
    "# Construct the command\n",
    "os.environ[\"PYTHONPATH\"] = os.path.abspath(\".\")  # Adjust according to your structure\n",
    "\n",
    "subprocess.run(\"pwd\")\n",
    "command = [\n",
    "    \"t5_mesh_transformer\",\n",
    "    f\"--model_dir={MODEL_DIR}\",\n",
    "    \"--gin_file=config/dataset.gin\",\n",
    "    \"--gin_file=config/t5.1.1.small.gin\",\n",
    "    \"--gin_param=utils.run.mesh_shape='model:1,batch:1'\",\n",
    "    \"--gin_param=utils.run.mesh_devices=['GPU:0']\",\n",
    "    f\"--gin_param=utils.run.save_checkpoints_steps={save_checkpoints_steps}\",\n",
    "    f\"--gin_param=utils.run.keep_checkpoint_max={keep_checkpoint_max}\",\n",
    "    f\"--gin_param=utils.run.train_steps={train_steps}\",\n",
    "    \"--module_import=config.mytask3\"\n",
    "]\n",
    "\n",
    "# Execute the command\n",
    "try:\n",
    "    result = subprocess.run(command, check=True, text=True, capture_output=True)\n",
    "    print(\"Output:\", result.stdout)\n",
    "    print(\"Errors:\", result.stderr)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"Output: {e.stdout}\")\n",
    "    print(f\"Errors: {e.stderr}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
