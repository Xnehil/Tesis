{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "# config = T5Config(\n",
    "#     d_model=512,  # Embedding dimension\n",
    "#     d_ff=2048,    # Feed-forward layer size\n",
    "#     num_layers=8,  # Number of encoder and decoder layers\n",
    "#     num_heads=8,   # Attention heads\n",
    "#     vocab_size=32128  # Size of the vocabulary\n",
    "# )\n",
    "\n",
    "# model = T5ForConditionalGeneration(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Load the vocabulary (token -> index) mapping\n",
    "vocab = {}  # You should populate this with your BPE vocabulary\n",
    "with open('tokenizadorIskonawa.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for idx, line in enumerate(vocab_file):\n",
    "        token, code = re.split(r'\\t', line.strip())\n",
    "        # Save as integer\n",
    "        vocab[token] = idx\n",
    "\n",
    "# Load the BPE tokenized dataset\n",
    "def load_bpe_dataset(file_path, vocab):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Tokenize each line into subwords and convert them to indices\n",
    "            tokens = line.strip().split()  # Assuming tokens are space-separated\n",
    "            token_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]  # Handle unknown tokens\n",
    "            dataset.append(token_ids)\n",
    "    return dataset\n",
    "\n",
    "bpe_tokenized_dataset = load_bpe_dataset('tokens.txt', vocab)\n",
    "\n",
    "# Check dataset example\n",
    "print(bpe_tokenized_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.2\n",
    "\n",
    "for split in ['eval', 'train']:\n",
    "    corpus = 'tokens.txt'\n",
    "    output = f'iskCorpus.t5.{split}.tsv'\n",
    "    with open(corpus, 'r') as reader:\n",
    "        lines = reader.readlines()\n",
    "        total_lines = len(lines)\n",
    "        split_index = int(total_lines * split_ratio)\n",
    "        \n",
    "        with open(output, 'w') as writer:\n",
    "            writer.write('input_text\\ttarget_text\\n')\n",
    "            for number, line in enumerate(lines):\n",
    "                if split == 'train' and number >= split_index:\n",
    "                    line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                    writer.write(line+'\\t'+line+'\\n')\n",
    "                elif split == 'eval' and number < split_index:\n",
    "                    line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                    writer.write(line+'\\t'+line+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "# import seqio\n",
    "# import t5.data\n",
    "# from t5.data import preprocessors\n",
    "# import tensorflow as tf\n",
    "# TaskRegistry = seqio.TaskRegistry\n",
    "# MixtureRegistry = seqio.MixtureRegistry\n",
    "\n",
    "# # Define your dataset splits\n",
    "# datasplit = {\n",
    "#     \"train\": \"iskCorpus.t5.train.tsv\",\n",
    "#     \"validation\": \"iskCorpus.t5.eval.tsv\"\n",
    "# }\n",
    "\n",
    "# DEFAULT_OUTPUT_FEATURES = {\n",
    "#     \"inputs\": seqio.Feature(\n",
    "#         seqio.SentencePieceVocabulary(vocab), add_eos=True,\n",
    "#         required=False, dtype=tf.int32),\n",
    "#     \"targets\": seqio.Feature(\n",
    "#         seqio.SentencePieceVocabulary(vocab), add_eos=True, dtype=tf.int32)\n",
    "# }\n",
    "\n",
    "# # Add the text generation tasks\n",
    "# TaskRegistry.add(\n",
    "#     \"text_generation_span_corruption\",\n",
    "#     source=seqio.TextLineDataSource(split_to_filepattern=datasplit),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(preprocessors.parse_tsv),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         preprocessors.span_corruption,  # 15% span corruption\n",
    "#         seqio.preprocessors.append_eos_after_trim,\n",
    "#     ],\n",
    "#     output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "#     metric_fns=[]  # Add evaluation metrics if needed\n",
    "# )\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"text_generation_iid_denoising\",\n",
    "#     source=seqio.TextLineDataSource(split_to_filepattern=datasplit),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(preprocessors.parse_tsv),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         preprocessors.iid_denoising,    # 15% i.i.d. denoising\n",
    "#         seqio.preprocessors.append_eos_after_trim,\n",
    "#     ],\n",
    "#     output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "#     metric_fns=[]  # Add evaluation metrics if needed\n",
    "# )\n",
    "\n",
    "# # Mixture of both tasks\n",
    "# MixtureRegistry.add(\n",
    "#     \"text_generation_mixture\",\n",
    "#     [\"text_generation_span_corruption\", \"text_generation_iid_denoising\"],\n",
    "#     default_rate=1.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 15:25:40.262617: I tensorflow/compiler/xla/stream_executor/tpu/tpu_initializer_helper.cc:269] Libtpu path is: libtpu.so\n",
      "2024-10-05 15:25:40.263152: I tensorflow/compiler/xla/stream_executor/tpu/tpu_initializer_helper.cc:277] Failed to open libtpu: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2024-10-05 15:25:40.272886: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.13.1\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 15:25:44.628138: I tensorflow/compiler/xla/stream_executor/tpu/tpu_platform_interface.cc:76] No TPU platform registered. Waiting 1 second and trying again... (4 tries left)\n",
      "2024-10-05 15:25:45.628280: I tensorflow/compiler/xla/stream_executor/tpu/tpu_platform_interface.cc:76] No TPU platform registered. Waiting 1 second and trying again... (3 tries left)\n",
      "2024-10-05 15:25:46.628410: I tensorflow/compiler/xla/stream_executor/tpu/tpu_platform_interface.cc:76] No TPU platform registered. Waiting 1 second and trying again... (2 tries left)\n",
      "2024-10-05 15:25:47.628542: I tensorflow/compiler/xla/stream_executor/tpu/tpu_platform_interface.cc:76] No TPU platform registered. Waiting 1 second and trying again... (1 tries left)\n",
      "2024-10-05 15:25:48.628654: I tensorflow/compiler/xla/stream_executor/tpu/tpu_platform_interface.cc:73] No TPU platform found.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "TPUs not found in the cluster. Failed in initialization: No matching devices found for '/device:TPU_SYSTEM:0' [Op:__inference__tpu_init_fn_4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/tpu/tpu_strategy_util.py:127\u001b[0m, in \u001b[0;36minitialize_tpu_system\u001b[0;34m(cluster_resolver)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(tpu\u001b[38;5;241m.\u001b[39m_tpu_system_device_name(job)):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43m_tpu_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: No matching devices found for '/device:TPU_SYSTEM:0' [Op:__inference__tpu_init_fn_4]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental_connect_to_cluster(resolver)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# This is the TPU initialization code that has to be at the beginning.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_tpu_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# strategy = tf.distribute.TPUStrategy(cluster_resolver)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# z = strategy.run(add_fn, args=(x,y))\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(z)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/tpu/tpu_strategy_util.py:130\u001b[0m, in \u001b[0;36minitialize_tpu_system\u001b[0;34m(cluster_resolver)\u001b[0m\n\u001b[1;32m    128\u001b[0m   context\u001b[38;5;241m.\u001b[39masync_wait()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 130\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPUs not found in the cluster. Failed in initialization: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m       \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_eagerly \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: TPUs not found in the cluster. Failed in initialization: No matching devices found for '/device:TPU_SYSTEM:0' [Op:__inference__tpu_init_fn_4]"
     ]
    }
   ],
   "source": [
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/harvymartinez16_gmail_com/Tesis/\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "@tf.function\n",
    "def add_fn(x,y):\n",
    "  z = x + y\n",
    "  return z\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\"local\")\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "# strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "\n",
    "# x = tf.constant(1.)\n",
    "# y = tf.constant(1.)\n",
    "# z = strategy.run(add_fn, args=(x,y))\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from t5 import models\n",
    "import gin\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Set up your environment variables and paths\n",
    "MODEL_DIR = \"t5_sl_small\"  # Change to your desired model directory\n",
    "# os.environ[\"PYTHONPATH\"] = f\"{os.environ['PYTHONPATH']}:/text-to-text-transfer-transformer:/text-to-text-transfer-transformer/t5\"\n",
    "\n",
    "# Define training parameters\n",
    "train_steps = 1000000\n",
    "save_checkpoints_steps = 40000\n",
    "keep_checkpoint_max = 2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up the gin configuration for the model and dataset\n",
    "# You can create your .gin files or configure them directly in the notebook.\n",
    "gin.parse_config_file(\"config/dataset.gin\")\n",
    "gin.parse_config_file(\"config/t5.1.1.small.gin\")\n",
    "gin.bind_parameter(\"utils.run.mesh_shape\", \"model:2,batch:2\")\n",
    "gin.bind_parameter(\"utils.run.mesh_devices\", [\"tpu:0\"])\n",
    "# gin.bind_parameter(\"MIXTURE_NAME\", \"mixture_iskonawa_test\")  # Your mixture name\n",
    "gin.bind_parameter(\"utils.run.save_checkpoints_steps\", save_checkpoints_steps)\n",
    "gin.bind_parameter(\"utils.run.keep_checkpoint_max\", keep_checkpoint_max)\n",
    "gin.bind_parameter(\"utils.run.train_steps\", train_steps)\n",
    "\n",
    "class ProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logger.info(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}\")\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 100 == 0:  # Log every 1000 steps\n",
    "            logger.info(f\"Step {batch}: loss = {logs.get('loss', 'N/A'):.4f}, accuracy = {logs.get('accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "# Start the training\n",
    "with strategy.scope():\n",
    "    models.t5_mesh_transformer(\n",
    "        model_dir=MODEL_DIR,\n",
    "        gin_file=\"/config/dataset.gin\",\n",
    "        gin_file=\"/config/t5.1.1.small.gin\",\n",
    "        module_import=\"/config/mytask3\",  \n",
    "        callbacks=[ProgressCallback()]\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
