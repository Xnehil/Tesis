{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "# config = T5Config(\n",
    "#     d_model=512,  # Embedding dimension\n",
    "#     d_ff=2048,    # Feed-forward layer size\n",
    "#     num_layers=8,  # Number of encoder and decoder layers\n",
    "#     num_heads=8,   # Attention heads\n",
    "#     vocab_size=32128  # Size of the vocabulary\n",
    "# )\n",
    "\n",
    "# model = T5ForConditionalGeneration(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Load the vocabulary (token -> index) mapping\n",
    "vocab = {}  # You should populate this with your BPE vocabulary\n",
    "with open('tokenizadorIskonawa.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for idx, line in enumerate(vocab_file):\n",
    "        token, code = re.split(r'\\t', line.strip())\n",
    "        # Save as integer\n",
    "        vocab[token] = idx\n",
    "\n",
    "# Load the BPE tokenized dataset\n",
    "def load_bpe_dataset(file_path, vocab):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Tokenize each line into subwords and convert them to indices\n",
    "            tokens = line.strip().split()  # Assuming tokens are space-separated\n",
    "            token_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]  # Handle unknown tokens\n",
    "            dataset.append(token_ids)\n",
    "    return dataset\n",
    "\n",
    "bpe_tokenized_dataset = load_bpe_dataset('tokens.txt', vocab)\n",
    "\n",
    "# Check dataset example\n",
    "print(bpe_tokenized_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.2\n",
    "\n",
    "for split in ['eval', 'train']:\n",
    "    corpus = 'tokens.txt'\n",
    "    output = f'iskCorpus.t5.{split}.tsv'\n",
    "    with open(corpus, 'r') as reader:\n",
    "        lines = reader.readlines()\n",
    "        total_lines = len(lines)\n",
    "        split_index = int(total_lines * split_ratio)\n",
    "        \n",
    "        with open(output, 'w') as writer:\n",
    "            writer.write('input_text\\ttarget_text\\n')\n",
    "            for number, line in enumerate(lines):\n",
    "                if split == 'train' and number >= split_index:\n",
    "                    line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                    writer.write(line+'\\t'+line+'\\n')\n",
    "                elif split == 'eval' and number < split_index:\n",
    "                    line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                    writer.write(line+'\\t'+line+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "# import seqio\n",
    "# import t5.data\n",
    "# from t5.data import preprocessors\n",
    "# import tensorflow as tf\n",
    "# TaskRegistry = seqio.TaskRegistry\n",
    "# MixtureRegistry = seqio.MixtureRegistry\n",
    "\n",
    "# # Define your dataset splits\n",
    "# datasplit = {\n",
    "#     \"train\": \"iskCorpus.t5.train.tsv\",\n",
    "#     \"validation\": \"iskCorpus.t5.eval.tsv\"\n",
    "# }\n",
    "\n",
    "# DEFAULT_OUTPUT_FEATURES = {\n",
    "#     \"inputs\": seqio.Feature(\n",
    "#         seqio.SentencePieceVocabulary(vocab), add_eos=True,\n",
    "#         required=False, dtype=tf.int32),\n",
    "#     \"targets\": seqio.Feature(\n",
    "#         seqio.SentencePieceVocabulary(vocab), add_eos=True, dtype=tf.int32)\n",
    "# }\n",
    "\n",
    "# # Add the text generation tasks\n",
    "# TaskRegistry.add(\n",
    "#     \"text_generation_span_corruption\",\n",
    "#     source=seqio.TextLineDataSource(split_to_filepattern=datasplit),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(preprocessors.parse_tsv),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         preprocessors.span_corruption,  # 15% span corruption\n",
    "#         seqio.preprocessors.append_eos_after_trim,\n",
    "#     ],\n",
    "#     output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "#     metric_fns=[]  # Add evaluation metrics if needed\n",
    "# )\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"text_generation_iid_denoising\",\n",
    "#     source=seqio.TextLineDataSource(split_to_filepattern=datasplit),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(preprocessors.parse_tsv),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         preprocessors.iid_denoising,    # 15% i.i.d. denoising\n",
    "#         seqio.preprocessors.append_eos_after_trim,\n",
    "#     ],\n",
    "#     output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "#     metric_fns=[]  # Add evaluation metrics if needed\n",
    "# )\n",
    "\n",
    "# # Mixture of both tasks\n",
    "# MixtureRegistry.add(\n",
    "#     \"text_generation_mixture\",\n",
    "#     [\"text_generation_span_corruption\", \"text_generation_iid_denoising\"],\n",
    "#     default_rate=1.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/harvymartinez16_gmail_com/Tesis/\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "# List and print GPU devices available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPU found.\")\n",
    "\n",
    "# Define a function for addition using @tf.function\n",
    "@tf.function\n",
    "def add_fn(x, y):\n",
    "    z = x + y\n",
    "    return z\n",
    "\n",
    "# Example of running the function\n",
    "x = tf.constant(2.0)\n",
    "y = tf.constant(3.0)\n",
    "result = add_fn(x, y)\n",
    "print(\"Result: \", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import t5\n",
    "import gin\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Set up your environment variables and paths\n",
    "MODEL_DIR = \"t5_sl_small\"  # Change to your desired model directory\n",
    "# os.environ[\"PYTHONPATH\"] = f\"{os.environ['PYTHONPATH']}:/text-to-text-transfer-transformer:/text-to-text-transfer-transformer/t5\"\n",
    "\n",
    "# Define training parameters\n",
    "train_steps = 1000000\n",
    "save_checkpoints_steps = 40000\n",
    "keep_checkpoint_max = 2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up the gin configuration for the model and dataset\n",
    "# You can create your .gin files or configure them directly in the notebook.\n",
    "gin.parse_config_file(\"config/dataset.gin\")\n",
    "gin.parse_config_file(\"config/t5.1.1.small.gin\")\n",
    "gin.bind_parameter(\"utils.run.mesh_shape\", \"model:1,batch:1\")\n",
    "gin.bind_parameter(\"utils.run.mesh_devices\", [\"GPU:0\"])\n",
    "# gin.bind_parameter(\"MIXTURE_NAME\", \"mixture_iskonawa_test\")  # Your mixture name\n",
    "gin.bind_parameter(\"utils.run.save_checkpoints_steps\", save_checkpoints_steps)\n",
    "gin.bind_parameter(\"utils.run.keep_checkpoint_max\", keep_checkpoint_max)\n",
    "gin.bind_parameter(\"utils.run.train_steps\", train_steps)\n",
    "\n",
    "class ProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logger.info(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}\")\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 100 == 0:  # Log every 1000 steps\n",
    "            logger.info(f\"Step {batch}: loss = {logs.get('loss', 'N/A'):.4f}, accuracy = {logs.get('accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy()  # Supports multi-GPU, defaults to single GPU if only one available\n",
    "\n",
    "# # Start the training\n",
    "# with strategy.scope():\n",
    "#     mesh_transformer(\n",
    "#         model_dir=MODEL_DIR,\n",
    "#         gin_file=[\"/config/dataset.gin\",\"/config/t5.1.1.small.gin\"],\n",
    "#         module_import=\"/config/mytask3\",  \n",
    "#         callbacks=[ProgressCallback()]\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# Define the model directory\n",
    "MODEL_DIR = \"t5_isk_small\"\n",
    "train_steps = 1000000\n",
    "save_checkpoints_steps = 40000\n",
    "keep_checkpoint_max = 2 \n",
    "# Construct the command\n",
    "os.environ[\"PYTHONPATH\"] = os.path.abspath(\".\")  # Adjust according to your structure\n",
    "\n",
    "subprocess.run(\"pwd\")\n",
    "command = [\n",
    "    \"t5_mesh_transformer\",\n",
    "    f\"--model_dir={MODEL_DIR}\",\n",
    "    \"--gin_file=config/dataset.gin\",\n",
    "    \"--gin_file=config/t5.1.1.small.gin\",\n",
    "    \"--gin_param=utils.run.mesh_shape='model:1,batch:1'\",\n",
    "    \"--gin_param=utils.run.mesh_devices=['GPU:0']\",\n",
    "    f\"--gin_param=utils.run.save_checkpoints_steps={save_checkpoints_steps}\",\n",
    "    f\"--gin_param=utils.run.keep_checkpoint_max={keep_checkpoint_max}\",\n",
    "    f\"--gin_param=utils.run.train_steps={train_steps}\",\n",
    "    \"--module_import=config.mytask3\"\n",
    "]\n",
    "\n",
    "# Execute the command\n",
    "try:\n",
    "    result = subprocess.run(command, check=True, text=True, capture_output=True)\n",
    "    print(\"Output:\", result.stdout)\n",
    "    print(\"Errors:\", result.stderr)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"Output: {e.stdout}\")\n",
    "    print(f\"Errors: {e.stderr}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
